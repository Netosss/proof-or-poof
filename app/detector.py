import logging
import time
import cv2
import numpy as np
import asyncio
import os
import json
import random
import re
from collections import OrderedDict 
from PIL import Image
from PIL.ExifTags import TAGS
from typing import Optional, Union
from app.c2pa_reader import get_c2pa_manifest
from app.runpod_client import run_deep_forensics
from gemini_client import analyze_image_pro_turbo, analyze_batch_images_pro_turbo
from app.security import security_manager

logger = logging.getLogger(__name__)

# --- TIERED AI SIGNATURE CONFIGURATION ---

# TIER 1: DEFINITE GENERATORS (99.9% AI)
TIER_1_GENERATORS = [
    "midjourney", "dall-e", "dalle", "stable diffusion", "stablediffusion",
    "adobe firefly", "bing image creator", "flux", "black forest labs",
    "ideogram", "leonardo.ai", "leonardo", "runway", "gen-2", "gen-3",
    "luma", "dream machine", "kling", "kolors", "pika labs", "sora",
    "tensor.art", "seaart", "civitai", "playground ai", "recraft",
    "comfyui", "automatic1111", "invokeai", "easydiffusion",
    "foocus", "mj v6", "nijijourney", "img2img", "text2img",
    "generative fill", "generative expand", "trainedalgorithmicmedia",
    "compositeWithTrainedAlgorithmicMedia", "algorithmicmedia"
]

# TIER 2: SUSPICIOUS TECHNICAL TERMS (High Risk)
TIER_2_TECH_TERMS = [
    "latent", "hypernetwork", "lora", "denoising", "sampler",
    "cfg scale", "steps: ", "seed:", "clip skip", "controlnet",
    "inpainting", "outpainting", "prompt", "negative prompt",
    "synthetic", "deep learning", "neural network", "gan",
    "generated by", "created with ai", "imagined with"
]

# TIER 3: THE "RISK TAKER" GENERAL SEARCH (Low Precision)
TIER_3_REGEX = r"(?i)\b(ai|gpt|bot|gen)\b"

# LRU Cache implementation for forensic results
class LRUCache:
    def __init__(self, capacity: int = 1000):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key):
        if key not in self.cache:
            return None
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

forensic_cache = LRUCache(capacity=1000)

def get_tiered_signature_score(full_dump: str, clean_dump: str) -> tuple:
    """
    Returns (score, signals_list) based on tiered metadata signatures.
    
    full_dump: Includes raw binary text scan (Tier 1 & 2 only)
    clean_dump: Includes only parsed EXIF/PIL info (Tier 3 only)
    """
    score = 0.0
    signals = []
    
    full_dump = full_dump.lower()
    clean_dump = clean_dump.lower()

    # --- TIER 1: THE SMOKING GUNS (Checked in everything) ---
    for word in TIER_1_GENERATORS:
        if word in full_dump:
            signals.append(f"Found definite generator signature: '{word}'")
            return 0.99, signals 

    # --- TIER 2: STRONG INDICATORS (Checked in everything) ---
    tier_2_hits = 0
    for word in TIER_2_TECH_TERMS:
        if word in full_dump:
            tier_2_hits += 1
            signals.append(f"Found technical artifact: '{word}'")
    
    if tier_2_hits > 0:
        score += 0.90 + ((tier_2_hits - 1) * 0.40)

    # --- TIER 3: THE "RISK TAKER" (Checked ONLY in Clean Metadata) ---
    if score < 0.99:
        # We only run the regex on the parsed metadata to avoid binary noise false positives
        matches = re.findall(TIER_3_REGEX, clean_dump)
        if matches:
            unique_matches = list(set(matches))
            score += 0.25 
            signals.append(f"Found generic AI keywords in metadata: {unique_matches}")

    return min(score, 0.99), signals

def get_image_hash(source: Union[str, Image.Image], fast_mode: bool = False) -> str:
    """
    Generate a secure SHA-256 hash of the image source (optimized with grayscale).
    fast_mode=True uses even smaller thumbnail for video frame caching.
    """
    if isinstance(source, str):
        with open(source, 'rb') as f:
            # Hash first 2MB for speed, but use secure method
            return security_manager.get_safe_hash(f.read(2048 * 1024))
    else:
        # For PIL Images: hash raw pixel bytes (no JPEG artifacts)
        thumb = source.copy()
        # Use 32x32 for video frames (fast_mode), 64x64 for standalone images
        size = (32, 32) if fast_mode else (64, 64)
        thumb.thumbnail(size)
        thumb = thumb.convert("L")  # Grayscale reduces data while preserving uniqueness
        # Hash raw pixel bytes directly (faster, no compression artifacts)
        return security_manager.get_safe_hash(np.array(thumb).tobytes())

def get_exif_data(file_path: str) -> dict:
    """
    Extract metadata from the image (EXIF for JPEG/TIFF, 'info' for PNG/WebP).
    Explicitly closed via 'with'.
    """
    try:
        with Image.open(file_path) as img:
            metadata = {}
            
            # 1. Standard EXIF (JPEG, TIFF, some WebP)
            exif = img._getexif()
            if exif:
                for tag, value in exif.items():
                    decoded = TAGS.get(tag, tag)
                    metadata[decoded] = value
            
            # 2. PNG/WebP Metadata (Text chunks, etc.)
            # These formats often store info in the .info attribute instead of EXIF
            if hasattr(img, 'info') and img.info:
                for key, value in img.info.items():
                    # Handle ICC Profile specially
                    if key == "icc_profile":
                        metadata["icc_profile"] = "present"
                    # Avoid binary data bloat, only take string-like metadata
                    elif isinstance(key, str) and isinstance(value, (str, int, float)):
                        # Don't overwrite EXIF tags if they already exist
                        if key not in metadata:
                            metadata[key] = value
            
            return metadata
    except Exception:
        return {}

def is_frame_quality_ok(frame: np.ndarray, min_brightness: float = 20, min_sharpness: float = 50) -> tuple:
    """
    Check if frame is not too dark or blurry for reliable AI detection.
    Returns (is_ok, brightness, sharpness) for potential weighted aggregation.
    """
    try:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Quick brightness check first (fast, avoids expensive Laplacian for dark frames)
        brightness = np.mean(gray)
        if brightness < min_brightness:
            return False, brightness, 0.0
        
        # Check sharpness via Laplacian variance (avoid blurry frames)
        # Use smaller region for speed (center crop)
        h, w = gray.shape
        center_crop = gray[h//4:3*h//4, w//4:3*w//4]
        laplacian_var = cv2.Laplacian(center_crop, cv2.CV_64F).var()
        if laplacian_var < min_sharpness:
            return False, brightness, laplacian_var
        
        return True, brightness, laplacian_var
    except:
        return True, 128.0, 100.0  # Safe defaults if check fails

def extract_video_frames(video_path: str) -> tuple:
    """
    Extract 3 frames at 20%, 50%, 80% of video duration (Tri-Frame Strategy).
    Optimized for batch GPU processing - 3 frames process in ~same time as 1.
    Returns (frames, quality_rejected_count)
    """
    frames = []
    quality_rejected = 0
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return [], 0
            
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames <= 0:
            return [], 0
        
        # Tri-Frame: 20%, 50%, 80% (avoids intro/outro black frames)
        sample_points = [
            int(total_frames * 0.20),
            int(total_frames * 0.50),
            int(total_frames * 0.80)
        ]
        
        for pos in sample_points:
            cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
            ret, frame = cap.read()
            if ret:
                # Quality filter: skip dark/blurry frames
                is_ok, brightness, sharpness = is_frame_quality_ok(frame)
                if not is_ok:
                    quality_rejected += 1
                    continue
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame_rgb))
        
        cap.release()
        
        if quality_rejected > 0:
            logger.info(f"[VIDEO] Skipped {quality_rejected} low-quality frames (dark/blurry)")
        
        # Fallback if too many frames rejected
        if len(frames) < 1 and total_frames >= 1:
            cap = cv2.VideoCapture(video_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, int(total_frames * 0.5))
            ret, frame = cap.read()
            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame_rgb))
            cap.release()
            
    except Exception as e:
        logger.error(f"Error extracting video frames: {e}")
    
    return frames, quality_rejected

async def get_video_metadata(video_path: str) -> dict:
    """Extract video metadata using ffprobe (async to avoid blocking event loop)."""
    try:
        proc = await asyncio.create_subprocess_exec(
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_format', '-show_streams', video_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        try:
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=10)
            if proc.returncode == 0:
                return json.loads(stdout.decode())
        except asyncio.TimeoutError:
            proc.kill()
            await proc.wait()
            logger.warning("ffprobe timeout")
    except FileNotFoundError:
        logger.warning("ffprobe not installed")
    except Exception as e:
        logger.error(f"Error extracting video metadata: {e}")
    return {}

def get_video_metadata_score(metadata: dict, filename: str = "", file_path: str = "") -> tuple:
    """
    Analyze video metadata for human vs AI signals using soft thresholds.
    Returns (human_score, ai_score, signals, early_exit_label)
    
    early_exit_label: "human", "ai", or None (continue to frame analysis)
    """
    human_score = 0.0
    ai_score = 0.0
    signals = []
    
    if not metadata:
        return 0.0, 0.0, ["No metadata available"], None
    
    format_info = metadata.get("format", {})
    tags = format_info.get("tags", {})
    streams = metadata.get("streams", [])
    
    # Normalize tag keys to lowercase for consistent matching
    tags_lower = {k.lower(): v for k, v in tags.items()}
    
    # Get encoder info
    encoder = str(tags_lower.get("encoder", "")).lower()
    
    # === DEVICE MARKER DETECTION ===
    
    # Check for Android device markers
    has_android_marker = False
    android_markers = ["com.android.version", "com.android.capture", "com.samsung"]
    for marker in android_markers:
        for key in tags_lower:
            if marker in key.lower():
                has_android_marker = True
                signals.append(f"Android device marker: {key}")
                break
        if has_android_marker:
            break
    
    # Check for iOS/Apple device markers  
    has_ios_marker = False
    apple_markers = [
        "com.apple.quicktime.make",
        "com.apple.quicktime.model", 
        "com.apple.quicktime.software",
        "com.apple.quicktime.creationdate"
    ]
    for marker in apple_markers:
        if marker in tags_lower or marker.replace(".", "_") in tags_lower:
            has_ios_marker = True
            signals.append(f"Apple device marker: {marker}")
            break
    
    device_marker = has_android_marker or has_ios_marker
    
    # Check for FFmpeg/x264 encoding
    has_ffmpeg = "lavf" in encoder
    has_x264 = "x264" in encoder
    
    # Also check stream-level encoder tags
    for stream in streams:
        stream_tags = stream.get("tags", {})
        stream_encoder = str(stream_tags.get("encoder", "")).lower()
        if "x264" in stream_encoder:
            has_x264 = True
        if "lavf" in stream_encoder:
            has_ffmpeg = True
    
    # Binary check for x264 in file (it's often in H.264 private data, not in tags)
    # Only do this if we have ffmpeg but not x264, and we have a file path
    if file_path and has_ffmpeg and not has_x264:
        try:
            with open(file_path, 'rb') as f:
                # Read first 1MB (encoder string is usually near the start)
                chunk = f.read(1024 * 1024)
                if b'x264' in chunk:
                    has_x264 = True
        except:
            pass
    
    # === PRE-COMPUTE STREAM INFO (efficiency: single pass) ===
    video_stream = None
    audio_stream = None
    for stream in streams:
        if stream.get("codec_type") == "video" and video_stream is None:
            video_stream = stream
        elif stream.get("codec_type") == "audio" and audio_stream is None:
            audio_stream = stream
        if video_stream and audio_stream:
            break
    
    # Pre-compute common values
    duration = 0.0
    bit_rate = 0
    try:
        duration = float(format_info.get("duration", 0))
        bit_rate = int(format_info.get("bit_rate", 0))
    except:
        pass
    
    # FPS computation
    fps = 0.0
    avg_frame_rate = ""
    r_frame_rate = ""
    if video_stream:
        try:
            avg_frame_rate = video_stream.get("avg_frame_rate", "0/1")
            r_frame_rate = video_stream.get("r_frame_rate", "0/1")
            if "/" in avg_frame_rate:
                num, den = avg_frame_rate.split("/")
                fps = float(num) / float(den) if float(den) > 0 else 0
            else:
                fps = float(avg_frame_rate)
        except:
            fps = 0.0
    
    is_exact_round_fps = abs(fps - 30.0) < 0.001 or abs(fps - 60.0) < 0.001 or abs(fps - 24.0) < 0.001
    
    # Resolution
    width = video_stream.get("width", 0) if video_stream else 0
    height = video_stream.get("height", 0) if video_stream else 0
    
    # Handler names
    format_handler = tags_lower.get("handler_name", "").lower()
    make = tags_lower.get("make", "").lower()
    model = tags_lower.get("model", "").lower()
    
    # Stream-level handler
    stream_handler = ""
    if video_stream:
        stream_handler = str(video_stream.get("tags", {}).get("handler_name", "")).lower()
    
    # === HUMAN SIGNALS ===
    
    # 1. Device marker: STRONG human signal (+0.50)
    if device_marker:
        human_score += 0.50
    
    # 2. FPS analysis - Non-round FPS typical of real device
    if fps > 0 and not is_exact_round_fps:
        human_score += 0.10
        signals.append(f"Device-native FPS: {fps:.4f}")
    
    # 3. Device/Camera metadata present
    if make or model:
        human_score += 0.25
        signals.append("Device manufacturer/model metadata present")

    # 4. GPS/Location data (strong human signal)
    # The "Human Hardware" Location Whitelist
    # We use partial matching (e.g. "latitud" matches "GPSLatitude" and "Latitude")
    location_markers = [
        "gps",          # Covers EXIF (GPSLatitude) and XMP
        "location",     # Covers Apple (com.apple.quicktime.location)
        "coordinates",  # Covers generic parsers
        "xyz",          # CRITICAL: Covers Android Video standard (©xyz)
        "latitud",      # Covers bare "Latitude" tags (common in JSON dumps)
        "longitud"      # Covers bare "Longitude" tags
    ]
    
    # The Logic
    for key, value in tags_lower.items():
        if any(term in key for term in location_markers):
            # Check for valid data (not empty/zero)
            val_str = str(value).strip()
            if value and val_str not in ["0", "0.0", "+0.0000+000.0000/", "None", "[]"]:
                human_score += 0.35
                signals.append(f"GPS/Location data present: {key}")
                break
    
    # 5. Creation time with timezone (phones add this)
    creation_time = tags_lower.get("creation_time", "")
    if creation_time and ("+" in creation_time or "Z" in creation_time):
        human_score += 0.10
        signals.append("Creation time with timezone")
    
    # 6. Audio channel analysis - mono typical of phone
    if audio_stream:
        channels = audio_stream.get("channels", 0)
        if channels == 1:
            human_score += 0.05
            signals.append("Mono audio (phone typical)")
        elif channels == 2:
            ai_score += 0.05
            signals.append("Stereo audio")
    
    # 7. Rotation metadata - phones often record with rotation
    if video_stream:
        rotation = video_stream.get("tags", {}).get("rotate", "")
        side_data = video_stream.get("side_data_list", [])
        has_rotation = rotation or any(sd.get("rotation") for sd in side_data if isinstance(sd, dict))
        if has_rotation:
            human_score += 0.10
            signals.append("Video rotation metadata (phone typical)")
    
    # 8. Duration analysis - AI videos typically 3-10s, human videos vary
    if duration > 30:
        human_score += 0.05
        signals.append(f"Long duration ({duration:.1f}s) - human typical")
    elif 2 < duration <= 5:
        ai_score += 0.05
        signals.append(f"Short duration ({duration:.1f}s) - AI typical")
    
    # 9. Variable frame rate / VBR - common in phone recordings
    if avg_frame_rate and r_frame_rate and avg_frame_rate != r_frame_rate:
        human_score += 0.05
        signals.append("Variable frame rate detected (phone/screen rec typical)")
    
    # 10. Handler name analysis - can reveal recording device
    handler_to_check = stream_handler or format_handler
    if "core media" in handler_to_check or "apple" in handler_to_check:
        human_score += 0.15
        signals.append("Apple Core Media handler")
    elif "android" in handler_to_check or "media handler" in handler_to_check:
        human_score += 0.10
        signals.append("Android media handler")
    
    # 11. Bitrate analysis - AI videos often have very consistent/high bitrates
    if bit_rate > 0 and duration > 0:
        if duration < 15 and bit_rate > 15_000_000:  # >15 Mbps for short video
            ai_score += 0.05
            signals.append(f"High bitrate short video ({bit_rate//1000}kbps)")

    # === NEW: HDR / COLOR PROFILE CHECK (Strong Human Signal) ===
    # Real cameras (iPhone 12+, High-end Android) often shoot HDR/BT.2020
    # AI generators are almost exclusively SDR/BT.709 (Standard)
    if video_stream:
        color_primaries = video_stream.get("color_primaries", "unknown")
        color_transfer = video_stream.get("color_transfer", "unknown")
        
        # 1. BT.2020 (Wide Color Gamut) - Very strong human signal
        if "bt2020" in color_primaries:
            human_score += 0.45
            signals.append(f"Wide Color Gamut detected ({color_primaries})")
            
        # 2. HDR Transfer Functions (HLG or PQ)
        # "arib-std-b67" = HLG (iPhone HDR)
        # "smpte2084" = PQ (Samsung/Cinema HDR)
        if "arib-std-b67" in color_transfer or "smpte2084" in color_transfer:
            human_score += 0.45
            signals.append(f"HDR Transfer Function detected ({color_transfer})")

    # === NEW: CONTAINER BRAND CHECK ===
    major_brand = tags_lower.get("major_brand", "").lower()
    
    # "qt  " with spaces is Apple's signature
    if "qt  " in major_brand or "qt" == major_brand.strip():
        human_score += 0.18
        signals.append("Apple QuickTime Container (major_brand)")
        
    # "mp42" is common on Android/Sony cameras
    if "mp42" in major_brand:
        human_score += 0.14
        signals.append("Android/Camera Container (major_brand: mp42)")
    
    # === AI SIGNALS ===
    
    # 1. FFmpeg + x264 + NO device marker = STRONG AI signal (+0.50)
    if has_ffmpeg and has_x264 and not device_marker:
        ai_score += 0.50
        signals.append("FFmpeg/x264 encoding without device marker (AI typical)")
    elif has_ffmpeg and not device_marker:
        # Just FFmpeg without device marker is weaker signal
        ai_score += 0.15
        signals.append("FFmpeg encoder without device marker")
    
    # 2. Exact round FPS (30/60/24) = synthetic signal (+0.15)
    if is_exact_round_fps:
        ai_score += 0.15
        signals.append(f"Exact round FPS: {fps:.4f} (synthetic typical)")
    
    # 3. Known AI video generators in encoder
    ai_encoders = ["runway", "pika", "sora", "kling", "luma", "midjourney", 
                   "stable video", "deforum", "animatediff", "svd", "cogvideo", "gen-2"]
    for ai_enc in ai_encoders:
        if ai_enc in encoder:
            ai_score += 0.80
            signals.append(f"AI generator in encoder: {ai_enc}")
            break
    
    # 4. Known AI in filename: weak AI signal (+0.10)
    filename_lower = filename.lower()
    ai_filename_markers = ["sora", "runway", "pika", "kling", "luma", "midjourney", 
                           "stablediffusion", "cogvideo", "gen2", "animatediff"]
    for ai_name in ai_filename_markers:
        if ai_name in filename_lower:
            ai_score += 0.10
            signals.append(f"AI keyword in filename: {ai_name}")
            break
    
    # 5. No metadata at all (slightly suspicious for modern videos)
    if not tags:
        ai_score += 0.10
        signals.append("No metadata tags")
    
    # 6. Resolution analysis (using pre-computed width/height)
    if width > 0 and height > 0:
        # Square resolutions are common in AI
        if width == height and width in [512, 768, 1024, 1280]:
            ai_score += 0.15
            signals.append(f"AI-typical square resolution: {width}x{height}")
        
        # AI-typical resolutions (common render sizes)
        elif (width, height) in [(1280, 720), (704, 1280), (1024, 576), (576, 1024)]:
            ai_score += 0.10
            signals.append(f"AI-typical resolution: {width}x{height}")
        
        # Device-native resolutions (phone screens often have odd sizes)
        # These are real phone resolutions that AI generators don't use
        elif width > 1000 and height > 1000:
            aspect = width / height if height > 0 else 0
            # Very tall aspect (phone portrait) with non-standard width
            if 0.4 < aspect < 0.6 and width not in [720, 1080, 1280]:
                human_score += 0.05
                signals.append(f"Device-native resolution: {width}x{height}")
            # Very wide aspect (ultrawide screen recording)
            elif aspect > 2.0:
                human_score += 0.05
                signals.append(f"Ultrawide resolution: {width}x{height}")
    
    # Cap scores at 1.0
    human_score = min(1.0, human_score)
    ai_score = min(1.0, ai_score)
    
    # === EARLY EXIT LOGIC ===
    early_exit = None
    
    # Strong human signal: exit immediately if device marker + other cues
    if human_score >= 0.60 and ai_score < 0.30:
        early_exit = "human"
        signals.append(f"EARLY EXIT: Human (h={human_score:.2f}, ai={ai_score:.2f})")
    
    # Strong AI signal: exit if clear AI markers
    elif ai_score >= 0.70 and human_score < 0.20:
        early_exit = "ai"
        signals.append(f"EARLY EXIT: AI (h={human_score:.2f}, ai={ai_score:.2f})")
    
    return human_score, ai_score, signals, early_exit

def get_forensic_metadata_score(exif: dict) -> tuple:
    """
    Advanced forensic check for human sensor physics using weighted tiers.
    Includes type and range validation to prevent metadata spoofing.
    """
    score = 0.0
    signals = []

    def to_float(val):
        try: return float(val)
        except: return None

    # --- Tier 1: Device Provenance (Max 0.60) ---
    make = str(exif.get("Make", "")).lower()
    software = str(exif.get("Software", "")).lower()
    
    if any(m in make for m in ["apple", "google", "samsung", "sony", "canon", "nikon", "fujifilm", "panasonic", "olympus", "leica"]):
        score += 0.35  # Increased from 0.30
        signals.append("Trusted device manufacturer provenance")
    
    if any(s in software for s in ["hdr+", "ios", "android", "deep fusion", "one ui", "version", "lightroom", "capture one"]):
        score += 0.25
        signals.append("Validated vendor-specific camera pipeline")

    # --- Tier 2: Physical Camera Consistency (Max 0.45) ---
    # Signal 2.1: Exposure Time - Valid range: 0 < exp < 30s
    exp = to_float(exif.get("ExposureTime"))
    if exp is not None and 0 < exp < 30:
        score += 0.15  # Increased from 0.10
        signals.append("Physically valid exposure duration")
    
    # Signal 2.2: ISO Speed - Valid range: 50 < ISO < 102400
    iso = to_float(exif.get("ISOSpeedRatings"))
    if iso is not None and 50 <= iso <= 102400:
        score += 0.15  # Increased from 0.10
        signals.append("Realistic sensor sensitivity (ISO)")
        
    # Signal 2.3: Aperture/F-Number - Valid range: 0.95 < f < 32
    f_num = to_float(exif.get("FNumber"))
    if f_num is not None and 0.95 <= f_num <= 32:
        score += 0.15  # Increased from 0.10
        signals.append("Valid physical aperture geometry")

    # --- Tier 3: Temporal Authenticity (Max 0.10) ---
    if "DateTimeOriginal" in exif:
        score += 0.08  # Increased from 0.03
        signals.append("Temporal capture timestamp present")
        
    subsec = str(exif.get("SubSecTimeOriginal", exif.get("SubSecTimeDigitized", "")))
    if subsec and subsec.isdigit() and subsec not in ["000", "000000"]:
        score += 0.02
        signals.append("High-precision sensor timing")

    # --- Tier 4: Hardware Serial Numbers (Max 0.10) - Strong provenance ---
    # Camera body serial number - unique hardware ID
    body_serial = exif.get("BodySerialNumber", "")
    if body_serial and len(str(body_serial)) >= 6:
        score += 0.05
        signals.append(f"Camera body serial: {str(body_serial)[:8]}...")
    
    # Lens serial number - confirms physical lens
    lens_serial = exif.get("LensSerialNumber", "")
    if lens_serial and len(str(lens_serial)) >= 4:
        score += 0.05
        signals.append(f"Lens serial number present")

    # --- Tier 5: Lens & Flash Data (Max 0.10) ---
    # Lens model - AI won't have real lens info
    lens_model = exif.get("LensModel", "")
    if lens_model and len(str(lens_model)) > 3:
        score += 0.05
        signals.append(f"Lens model: {str(lens_model)[:30]}")
    
    # Flash data - physical sensor event
    flash = exif.get("Flash")
    if flash is not None and flash > 0:
        score += 0.05
        signals.append("Flash sensor event recorded")

    # --- Tier 6: GPS Data (Max 0.08) - Strong hardware provenance ---
    # GPS coordinates
    if "GPSLatitude" in exif or "GPSLongitude" in exif:
        score += 0.03
        signals.append("GPS coordinates present")
    
    # GPS altitude - very strong human signal (requires actual GPS hardware)
    gps_alt = exif.get("GPSAltitude")
    if gps_alt is not None:
        try:
            alt = float(gps_alt) if not isinstance(gps_alt, tuple) else float(gps_alt[0]) / float(gps_alt[1])
            if -500 < alt < 10000:  # Valid altitude range
                score += 0.03
                signals.append(f"GPS altitude: {alt:.0f}m")
        except:
            pass
    
    # GPS timestamp - separate from photo timestamp, confirms GPS fix
    if "GPSDateStamp" in exif or "GPSTimeStamp" in exif:
        score += 0.02
        signals.append("GPS timestamp present")

    # --- Tier 7: JPEG Structure (Max 0.10) ---
    if "JPEGInterchangeFormat" in exif:
        score += 0.05
        signals.append("Firmware-level segment tables")
        
    if exif.get("Compression") in [6, 1]: 
        score += 0.05
        signals.append("Standard camera compression")

    # === TIER 8: COLOR PROFILES (Strong Human Signal) ===
    # Real phones/cameras often use "Uncalibrated" (65535) + ICC Profile (e.g. Display P3)
    # AI generators typically default to standard sRGB (1) with no profile.
    
    color_space = exif.get("ColorSpace")
    
    # 1. Check for "Uncalibrated" / Wide Gamut Signal
    # Value 65535 often means "Look at the ICC Profile" (common in Apple Display P3)
    if color_space == 65535:
        score += 0.20
        signals.append("Wide Gamut / Uncalibrated Color Space (Human Typical)")

    # 2. Check for ICC Profile presence (Pillow specific)
    # Most raw AI generations do not embed complex ICC profiles to save space.
    if "icc_profile" in exif:
         score += 0.15
         signals.append("ICC Color Profile detected")

    return round(score, 2), signals

def get_ai_suspicion_score(exif: dict, width: int = 0, height: int = 0, file_size: int = 0) -> tuple:
    """
    Weighted AI suspicion score based on blatant signatures, missing camera metadata,
    and image characteristics (dimensions, file size).
    """
    score = 0.0
    signals = []
    
    has_camera_info = exif.get("Make") or exif.get("Model")
    
    # 1. Hard AI Evidence (Software/Make keywords)
    ai_keywords = ["stable", "diffusion", "midjourney", "dalle", "flux", "sora", "kling", "firefly", "generative", "artificial"]
    software = str(exif.get("Software", "")).lower()
    make = str(exif.get("Make", "")).lower()
    
    # PNG/XMP Bridge: AI tools often store signatures in XMP for PNG files
    if not software and "XML:com.adobe.xmp" in exif:
        software = str(exif.get("XML:com.adobe.xmp", "")).lower()
    
    if any(k in software for k in ai_keywords):
        score += 0.40
        # For XMP, we truncate the log to avoid bloat
        log_software = software[:50] + "..." if len(software) > 50 else software
        signals.append(f"AI software signature detected: {log_software}")
    elif any(k in make for k in ai_keywords):
        score += 0.40
        signals.append(f"AI manufacturer signature: {make}")

    # 2. Missing Metadata (statistically unlikely for real cameras)
    if not has_camera_info:
        score += 0.03
        signals.append("Missing camera hardware provenance")

    if "DateTimeOriginal" not in exif:
        score += 0.02
        signals.append("Missing capture timestamp")

    if not exif.get("SubSecTimeOriginal") and not exif.get("SubSecTimeDigitized"):
        score += 0.03
        signals.append("Missing high-precision sensor timing")

    if "JPEGInterchangeFormat" not in exif:
        score += 0.05
        signals.append("Non-standard JPEG segment structure")

    # 3. AI-typical dimensions (only if no camera info)
    if width > 0 and height > 0 and not has_camera_info:
        # AI generators often use power-of-2 or specific widths
        ai_typical_widths = [512, 768, 1024, 1536, 2048]
        if width in ai_typical_widths:
            score += 0.15
            signals.append(f"AI-typical width: {width}px")
        elif height in ai_typical_widths:
            score += 0.15  # Same weight as width - equally suspicious
            signals.append(f"AI-typical height: {height}px")
        
        # Small images with AI-typical dimensions are highly suspicious
        # (Real photos are rarely this small AND have power-of-2 dimensions)
        total_pixels = width * height
        if total_pixels < 500000 and (width in ai_typical_widths or height in ai_typical_widths):
            score += 0.10
            signals.append(f"Small image ({total_pixels//1000}K pixels) with AI-typical dimensions")
        
        # 4. Non-standard aspect ratio (not 4:3, 3:2, 16:9, 1:1)
        aspect = width / height if height > 0 else 0
        standard_aspects = [1.0, 1.33, 1.5, 1.78, 0.75, 0.67, 0.56, 1.0]  # Common camera ratios
        is_standard = any(abs(aspect - std) < 0.08 for std in standard_aspects)
        
        # 4a. SCREENSHOT PROTECTION LOGIC
        # If it looks like a phone screenshot, reduce AI suspicion to force GPU scan.
        # We don't want to auto-flag screenshots as AI, we want to analyze their pixels.
        phone_widths = range(640, 1500)  # Common phone widths (iPhone/Android viewports)
        is_portrait = height > width
        is_phone_width = width in phone_widths
        is_phone_aspect = 0.40 < aspect < 0.60  # Modern phones: 9:16 to 9:22
        
        if is_portrait and is_phone_width and is_phone_aspect:
            # Likely a phone screenshot - reduce AI suspicion significantly
            score -= 0.20
            signals.append(f"Likely phone screenshot ({width}x{height}) - forcing GPU analysis")
        elif not is_standard and aspect > 0:
            score += 0.05
            signals.append(f"Non-standard aspect ratio: {aspect:.2f}")

    # 5. File size analysis (small file for resolution = likely AI/compressed)
    if width > 0 and height > 0 and file_size > 0 and not has_camera_info:
        pixels = width * height
        bytes_per_pixel = file_size / pixels if pixels > 0 else 0
        # Real photos typically have 0.5-3 bytes per pixel
        # AI images are often heavily compressed or have lower detail
        if bytes_per_pixel < 0.15 and pixels > 500000:
            score += 0.10
            signals.append(f"Low bytes/pixel: {bytes_per_pixel:.2f} (heavily compressed)")
        elif bytes_per_pixel < 0.3 and pixels > 500000:
            score += 0.05
            signals.append(f"Compressed image: {bytes_per_pixel:.2f} bytes/pixel")

    return round(min(score, 1.0), 2), signals

def boost_score(score: float, is_ai_likely: bool = True) -> float:
    """
    Boost confidence only for AI-likely results.
    Human-likely results keep their raw confidence to avoid misleading scores.
    """
    if is_ai_likely:
        return max(0.85, score)
    return score  # No boost for human results

async def detect_ai_media(file_path: str, trusted_metadata: dict = None) -> dict:
    """
    Final Optimized Consensus Engine.
    
    Args:
        file_path: Path to the media file
        trusted_metadata: Optional sidecar metadata from mobile device.
            Bypasses OS privacy stripping. Fields: Make, Model, Software, 
            DateTime, width, height, fileSize, namingEntropy, isOriginalPath, etc.
    """
    total_start = time.perf_counter()
    
    l1_data = {
        "status": "not_found",
        "provider": None,
        "description": "No cryptographic signature found."
    }

    # --- 1️⃣ LAYER 1: C2PA ---
    t_c2pa = time.perf_counter()
    manifest = get_c2pa_manifest(file_path)
    c2pa_time_ms = (time.perf_counter() - t_c2pa) * 1000
    logger.info(f"[TIMING] Layer 1 - C2PA check: {c2pa_time_ms:.2f}ms")
    if manifest:
        gen_info = manifest.get("claim_generator_info", [])
        generator = gen_info[0].get("name", "Unknown AI") if gen_info else manifest.get("claim_generator", "Unknown AI")

        is_generative_ai = False
        assertions = manifest.get("assertions", [])
        for assertion in assertions:
            if assertion.get("label") == "c2pa.actions.v2":
                actions = assertion.get("data", {}).get("actions", [])
                for action in actions:
                    source_type = action.get("digitalSourceType", "")
                    if "trainedAlgorithmicMedia" in source_type:
                        is_generative_ai = True
                    desc = action.get("description", "").lower()
                    if any(term in desc for term in ["generative fill", "ai-modified", "edited with ai", "ai generated"]):
                        is_generative_ai = True
            if is_generative_ai: break

        l1_data = {
            "status": "verified_ai" if is_generative_ai else "verified_human",
            "provider": generator,
            "description": f"Verified AI signature found ({generator})." if is_generative_ai else "Verified human-captured content."
        }

        return {
            "summary": "AI-Generated" if is_generative_ai else "No AI Detected",
            "confidence_score": 1.0,
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "flagged" if is_generative_ai else "passed",
                    "label": "Digital Signature",
                    "detail": f"Content Credentials confirm {'AI origin' if is_generative_ai else 'authentic origin'} ({generator})."
                }
            ]
        }

    is_video = file_path.lower().endswith(('.mp4', '.mov', '.avi', '.mkv', '.webm'))
    
    if is_video:
        safe_path = security_manager.sanitize_log_message(file_path)
        filename = os.path.basename(file_path)
        logger.info(f"Detecting AI in video: {safe_path}")
        
        # --- VIDEO METADATA EARLY EXIT ---
        t_video_meta = time.perf_counter()
        video_metadata = await get_video_metadata(file_path)
        human_score, ai_meta_score, meta_signals, early_exit = get_video_metadata_score(video_metadata, filename, file_path)
        video_meta_time_ms = (time.perf_counter() - t_video_meta) * 1000
        logger.info(f"[TIMING] Video metadata extraction: {video_meta_time_ms:.2f}ms")
        logger.info(f"[VIDEO META] Human score: {human_score:.2f}, AI score: {ai_meta_score:.2f}")
        logger.info(f"[VIDEO META] Signals: {meta_signals}")
        logger.info(f"[VIDEO META] Early exit: {early_exit}")
        
        # Early exit: Strong human metadata (camera/phone recording)
        if early_exit == "human":
            logger.info(f"[VIDEO] Early exit: Verified Human via metadata (h={human_score:.2f}, ai={ai_meta_score:.2f})")
            return {
                "summary": "No AI Detected",
                "confidence_score": 0.99,  # Max 99% without C2PA
                "evidence_chain": [
                    {
                        "layer": "Layer 1: Metadata Check",
                        "status": "passed",
                        "label": "Device Metadata",
                        "detail": f"Valid video metadata found ({meta_signals[0] if meta_signals else 'Camera/Phone'})."
                    }
                ]
            }
        
        # Early exit: Strong AI metadata (known AI generator)
        if early_exit == "ai":
            logger.info(f"[VIDEO] Early exit: AI Generator detected via metadata (h={human_score:.2f}, ai={ai_meta_score:.2f})")
            return {
                "summary": "AI-Generated",
                "confidence_score": 0.99,  # Max 99% without C2PA
                "evidence_chain": [
                    {
                        "layer": "Layer 1: Metadata Check",
                        "status": "flagged",
                        "label": "Software Signature",
                        "detail": f"AI generator signature detected in metadata ({meta_signals[0] if meta_signals else 'Unknown'})."
                    }
                ]
            }
        
        # No early exit - proceed to frame analysis (Tri-Frame Batch Strategy)
        logger.info(f"[VIDEO] No early exit, proceeding to tri-frame batch analysis...")
        
        # Run blocking video extraction in thread pool
        loop = asyncio.get_running_loop()
        frames, quality_rejected = await loop.run_in_executor(
            None, extract_video_frames, file_path
        )
        if not frames:
            return {
                "summary": "Analysis Failed",
                "confidence_score": 0.0,
                "evidence_chain": [
                    {
                        "layer": "System",
                        "status": "warning",
                        "label": "Video Error",
                        "detail": "Could not extract frames from video."
                    }
                ]
            }
        
        logger.info(f"[VIDEO] Extracted {len(frames)} frames (rejected {quality_rejected} low-quality)")
        
        # SINGLE BATCH REQUEST to Gemini (Layer 3.3)
        # Use analyze_batch_images_pro_turbo which returns aggregated median result
        # Run in thread pool to avoid blocking async event loop
        gemini_result = await loop.run_in_executor(
            None, analyze_batch_images_pro_turbo, frames
        )
        
        confidence = gemini_result.get("confidence", 0.0)
        explanation = gemini_result.get("explanation", "Analysis completed.")
        gpu_time_ms = 0 # No RunPod GPU used
        
        logger.info(f"[VIDEO] Gemini Batch Result: confidence={confidence}, explanation='{explanation}'")

        is_ai_likely = confidence > 0.5
        summary = "Likely AI-Generated" if is_ai_likely else "Likely Authentic"

        return {
            "summary": summary,
            "confidence_score": confidence,
            "gpu_time_ms": gpu_time_ms,
            "is_gemini_used": True,
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "warning",
                    "label": "Metadata Check",
                    "detail": "No definitive camera metadata found."
                },
                {
                    "layer": "Layer 3: Visual Context",
                    "status": "flagged" if is_ai_likely else "passed",
                    "label": "Visual Inspection",
                    "detail": explanation
                }
            ]
        }
    else:
        return await detect_ai_media_image_logic(file_path, l1_data, trusted_metadata=trusted_metadata)

# List of tags that indicate actual PHYSICAL human hardware provenance.
# If these are missing, the image is "Stripped" and goes to Gemini (if large).
# We exclude "Software", "XMP", or "Comments" from here because web tools add those.
PROVENANCE_WHITELIST = {
    "Make", "Model", "ExposureTime", "ISOSpeedRatings", 
    "FNumber", "GPSLatitude", "GPSLongitude", "GPSAltitude",
    "BodySerialNumber", "LensSerialNumber", "LensModel", "Flash",
    "SubSecTimeOriginal", "SubSecTimeDigitized"
}

async def detect_ai_media_image_logic(
    file_path: Optional[str], 
    l1_data: dict = None, 
    frame: Image.Image = None,
    trusted_metadata: dict = None
) -> dict:
    """
    Core consensus logic for images and video frames.
    """
    layer_start = time.perf_counter()
    
    if l1_data is None:
        l1_data = {"status": "not_found", "provider": None, "description": "N/A"}

    # --- EXIF Extraction ---
    t_exif = time.perf_counter()
    file_size = 0
    if frame:
        img_for_res = frame
        exif = {} 
        source_for_hash = frame
        source_path = None
        width, height = img_for_res.size
    else:
        exif = get_exif_data(file_path)
        try:
            with Image.open(file_path) as img:
                width, height = img.size
            source_for_hash = file_path
            source_path = file_path
            file_size = os.path.getsize(file_path)
        except:
            return {
                "summary": "Analysis Failed",
                "confidence_score": 0.0,
                "evidence_chain": [
                    {
                        "layer": "System",
                        "status": "warning",
                        "label": "File Error",
                        "detail": "Invalid image file - could not open."
                    }
                ]
            }
    
    # --- Merge Trusted Metadata (Sidecar) ---
    if trusted_metadata:
        logger.info(f"[SIDECAR] Using trusted metadata from device")
        for key in ["Make", "Model", "Software", "DateTime", "LensModel"]:
            if key in trusted_metadata:
                mapped_key = "DateTimeOriginal" if key == "DateTime" else key
                exif[mapped_key] = trusted_metadata[key]
        
        if "width" in trusted_metadata and "height" in trusted_metadata:
            width, height = trusted_metadata["width"], trusted_metadata["height"]
        if "fileSize" in trusted_metadata:
            file_size = trusted_metadata["fileSize"]
            
    exif_time_ms = (time.perf_counter() - t_exif) * 1000
    logger.info(f"[TIMING] EXIF extraction: {exif_time_ms:.2f}ms")

    # --- Log Truncated Metadata for Debugging ---
    # Slim the log: only first 20 chars of values, exclude huge binary blobs
    slim_log = {k: (str(v)[:20] + "..." if len(str(v)) > 20 else v) for k, v in exif.items()}
    logger.info(f"[META] Raw Metadata (Slim): {slim_log}")
    
    # --- Metadata Aggregation & Tiered Analysis ---
    # We stringify values to handle non-JSON serializable types like IFDRational or bytes
    clean_metadata = f" {json.dumps({k: str(v) for k, v in exif.items()})} "
    full_dump = clean_metadata
    
    # 1. Raw Packet Scan (First 50KB) - Added to full_dump only
    if not frame and file_path and os.path.exists(file_path):
        try:
            with open(file_path, 'rb') as f:
                raw_header = f.read(50000)
                full_dump += raw_header.decode('utf-8', errors='ignore')
        except Exception as e:
            logger.warning(f"Raw scan failed: {e}")
    
    # 3. Tiered Analysis (full_dump for T1/T2, clean_metadata for T3)
    tiered_score, tiered_signals = get_tiered_signature_score(full_dump, clean_metadata)
    
    # --- Metadata Scoring (Heuristics) ---
    t_scoring = time.perf_counter()
    human_score, human_signals = get_forensic_metadata_score(exif)
    base_ai_score, ai_signals = get_ai_suspicion_score(exif, width, height, file_size)
    
    # Combine results: Add tiered score to base heuristic score
    ai_score = min(0.99, base_ai_score + tiered_score)
    if tiered_signals:
        ai_signals.extend(tiered_signals)
        
    scoring_time_ms = (time.perf_counter() - t_scoring) * 1000
    logger.info(f"[TIMING] Metadata scoring: {scoring_time_ms:.2f}ms (human={human_score:.2f}, ai={ai_score:.2f})")
    
    if human_signals:
        logger.info(f"[META] Human signals: {human_signals}")
    if ai_signals:
        logger.info(f"[META] AI signals: {ai_signals}")
    
    # 1. VERIFIED HUMAN (Early Exit) - Optimized for non-adversarial users
    # Non-adversarial assumption: If metadata looks like real camera, trust it
    if human_score >= 0.60:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: High confidence human metadata ({human_score:.2f})")
        return {
            "summary": "Likely Authentic",
            "confidence_score": 0.99,  # Max 99% without C2PA
            "gpu_time_ms": 0,  # $0.00 cost
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "passed",
                    "label": "Device Metadata",
                    "detail": f"Valid camera metadata found ({exif.get('Make', 'Unknown')})."
                }
            ]
        }

    # 2. LIKELY HUMAN (Weaker signals but still skip GPU)
    if human_score >= 0.40 and ai_score < 0.15:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: Likely human metadata ({human_score:.2f}, ai={ai_score:.2f})")
        return {
            "summary": "Likely Authentic",
            "confidence_score": 0.9,
            "gpu_time_ms": 0,  # $0.00 cost
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "passed",
                    "label": "Device Metadata",
                    "detail": f"Heuristic analysis suggests authentic origin ({exif.get('Make', 'Unknown')})."
                }
            ]
        }

    # 3. LIKELY AI (Early Exit) - Strong AI signals in metadata
    if ai_score >= 0.50:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: High AI suspicion in metadata ({ai_score:.2f})")
        return {
            "summary": "Likely AI-Generated",
            "confidence_score": 0.95,
            "gpu_time_ms": 0,  # $0.00 cost
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "flagged",
                    "label": "Software Signature",
                    "detail": f"AI generation software detected ({exif.get('Software', 'AI Generator')})."
                }
            ]
        }

    # 4. SUSPICIOUS AI (Early Exit) - AI indicators + zero human signals
    if ai_score >= 0.38 and human_score == 0.0:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: AI indicators + no human metadata (ai={ai_score:.2f}, human={human_score:.2f})")
        suspicious_confidence = round(random.uniform(0.80, 0.90), 2)
        return {
            "summary": "AI-Generated",
            "confidence_score": suspicious_confidence,
            "gpu_time_ms": 0,  # $0.00 cost
            "evidence_chain": [
                {
                    "layer": "Layer 1: Metadata Check",
                    "status": "warning",
                    "label": "Metadata Check",
                    "detail": "No camera metadata found."
                },
                {
                    "layer": "Layer 2: Technical Heuristics",
                    "status": "flagged",
                    "label": "Image Structure",
                    "detail": "Dimensions typical of AI generation."
                }
            ]
        }

    # 5. AMBIGUOUS -> Forensic Scan (Gemini or Local Worker)
    total_pixels = width * height
    
    # Determine if the image is truly stripped of hardware provenance
    # We ignore "Web Junk" tags (JFIF, DPI, etc.) and only look for the High Value Whitelist
    # Also skip Gemini if we found strong technical AI indicators (Tier 1 or Tier 2)
    is_stripped = not any(tag in exif for tag in PROVENANCE_WHITELIST) and tiered_score < 0.50
    
    if is_stripped:
        logger.info(f"[META] Image classified as STRIPPED (No High-Value Provenance Tags found)")
    elif tiered_score >= 0.50:
        logger.info(f"[META] Image has technical AI signatures (score={tiered_score:.2f}) - bypassing stripped check")
    else:
        # Log which provenance tags were actually found
        found_tags = [tag for tag in PROVENANCE_WHITELIST if tag in exif]
        logger.info(f"[META] Image has PROVENANCE tags: {found_tags}")

    # Use fast_mode for video frames (smaller hash thumbnail for speed)
    img_hash = get_image_hash(source_for_hash, fast_mode=(frame is not None))
    cached_result = forensic_cache.get(img_hash)
    
    if cached_result is not None:
        logger.info(f"[CACHE] Hit for image scan")
        # Ensure we use the cached values
        forensic_probability = cached_result.get("ai_score", 0.0)
        actual_gpu_time_ms = 0.0
        # If it was a Gemini result, it will have this flag
        is_gemini_used = cached_result.get("is_gemini_used", False)
        
        if is_gemini_used:
            is_ai_likely = forensic_probability > 0.5
            raw_conf = forensic_probability if is_ai_likely else (1.0 - forensic_probability)
            final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)
            
            cached_explanation = cached_result.get("explanation", "Analyzed via second layer of AI analysis (Cached)")
            
            return {
                "summary": "Likely AI-Generated" if is_ai_likely else "No AI Detected",
                "confidence_score": round(final_conf, 2),
                "is_gemini_used": True,
                "is_cached": True,
                "gpu_time_ms": 0,
                "evidence_chain": [
                    {
                        "layer": "Layer 1: Metadata Check",
                        "status": "warning",
                        "label": "Metadata Check",
                        "detail": "No camera metadata found."
                    },
                    {
                        "layer": "Layer 3: Visual Context",
                        "status": "flagged" if is_ai_likely else "passed",
                        "label": "Visual Inspection",
                        "detail": cached_explanation
                    }
                ]
            }
    else:
        # --- STRIPPED-JPEG ADAPTIVE POLICY (Gemini Gateway) ---
        # Trigger Gemini if:
        # 1. Image is Stripped (no metadata) OR
        # 2. Metadata shows "uncertain" AI signals (0.3 < tiered_score < 0.8) - Double Check
        # AND Image is large enough (>= 50k pixels, approx 224x224)
        should_use_gemini = total_pixels >= 50_000 and (is_stripped or (0.3 < tiered_score < 0.8))
        
        if should_use_gemini:
            logger.info(f"[GEMINI] Triggering Gemini Pro Turbo (Pixels: {total_pixels}, Stripped: {is_stripped}, Score: {tiered_score:.2f})")
            
            gemini_res = analyze_image_pro_turbo(frame or file_path)
            logger.info(f"[GEMINI] Raw response: {json.dumps(gemini_res)}")
            
            gemini_score = float(gemini_res.get("confidence", -1.0))
            gemini_explanation = gemini_res.get("explanation", "Analyzed via second layer of AI analysis")
            
            if gemini_score >= 0.0:
                # Cache the Gemini result too!
                forensic_cache.put(img_hash, {
                    "ai_score": gemini_score,
                    "explanation": gemini_explanation,
                    "is_gemini_used": True,
                    "gpu_time_ms": 0
                })
                
                is_ai_likely = gemini_score > 0.5
                raw_conf = gemini_score if is_ai_likely else (1.0 - gemini_score)
                final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)

                return {
                    "summary": "Likely AI-Generated" if is_ai_likely else "Likely No AI Detected",
                    "confidence_score": round(final_conf, 2),
                    "is_gemini_used": True,
                    "gpu_time_ms": 0,
                    "evidence_chain": [
                        {
                            "layer": "Layer 1: Metadata Check",
                            "status": "warning",
                            "label": "Metadata Check",
                            "detail": "No camera metadata found."
                        },
                        {
                            "layer": "Layer 3: Visual Context",
                            "status": "flagged" if is_ai_likely else "passed",
                            "label": "Visual Inspection",
                            "detail": gemini_explanation
                        }
                    ]
                }

    # Wallet Guard: Prevent multi-minute GPU jobs for huge files
    if not frame and os.path.exists(file_path):
        f_size = os.path.getsize(file_path)
        if f_size > 50 * 1024 * 1024:
            return {
                "summary": "File too large to scan",
                "confidence_score": 0.0, 
                "evidence_chain": [
                    {
                        "layer": "System",
                        "status": "warning",
                        "label": "File Error",
                        "detail": "File exceeds size limit."
                    }
                ]
            }

    # --- Local GPU Scan (Fallback or Standard Case) ---
    t_gpu = time.perf_counter()
    # At this point, we know cached_result is None because the cache hit handled above
    forensic_result = await run_deep_forensics(source_for_hash, width, height)
    forensic_probability = forensic_result.get("ai_score", 0.0)
    actual_gpu_time_ms = forensic_result.get("gpu_time_ms", 0.0)
    forensic_cache.put(img_hash, forensic_result)
    roundtrip_ms = (time.perf_counter() - t_gpu) * 1000
    logger.info(f"[TIMING] Layer 2 - GPU scan (RunPod): {roundtrip_ms:.2f}ms | Actual GPU: {actual_gpu_time_ms:.2f}ms")
    
    total_layer_time_ms = (time.perf_counter() - layer_start) * 1000
    logger.info(f"[TIMING] Layer 2 - Total: {total_layer_time_ms:.2f}ms | Result: {forensic_probability:.4f}")
    
    # --- Metadata-Model Conflict Resolution (Weighted Blend) ---
    # When metadata signals strongly suggest AI but model disagrees, blend the scores
    final_signals = ["Multi-layered consensus applied (Deep Learning + FFT)"]
    original_prob = forensic_probability
    
    if ai_score >= 0.40 and forensic_probability < 0.20:
        # Conflict: Metadata screams AI, model says human
        # Blend: 65% metadata suspicion, 35% model (gentle nudge)
        blended_prob = (ai_score * 0.65) + (forensic_probability * 0.35)
        forensic_probability = blended_prob
        final_signals.append(f"Metadata-model conflict: blended {ai_score:.2f}*0.65 + {original_prob:.4f}*0.35 = {blended_prob:.4f}")
        logger.info(f"[CONFLICT] Metadata ai_score={ai_score:.2f}, model={original_prob:.4f} → blended={blended_prob:.4f}")
    
    l2_data = {
        "status": "detected" if forensic_probability > 0.5 else "not_detected",
        "probability": round(forensic_probability, 4),  # RAW probability for video aggregation
        "signals": final_signals
    }
    
    if forensic_probability > 0.5: summary = "AI-Generated"
    else: summary = "No AI Detected"
    
    # Boost the overall confidence score (only for AI-likely results)
    is_ai_likely = forensic_probability > 0.5
    raw_conf = forensic_probability if is_ai_likely else (1.0 - forensic_probability)
    final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)
    
    # Cap probabilistic scores at 0.99 to avoid "fake" 100% look, unless it's a hard metadata match
    if final_conf > 0.99:
        final_conf = 0.99

    return {
        "summary": summary,
        "confidence_score": round(final_conf, 2),
        "is_cached": cached_result is not None,
        "gpu_time_ms": actual_gpu_time_ms,  # Actual GPU time for cost calculation
        "evidence_chain": [
            {
                "layer": "Layer 1: Metadata Check",
                "status": "warning",
                "label": "Metadata Check",
                "detail": "No camera metadata found."
            },
            {
                "layer": "Layer 3: Deep Forensics",
                "status": "flagged" if is_ai_likely else "passed",
                "label": "Pixel Analysis",
                "detail": "Noise patterns consistent with generative AI." if is_ai_likely else "Sensor noise patterns consistent with optical lenses."
            }
        ]
    }