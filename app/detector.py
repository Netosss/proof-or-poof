import logging
import time
import cv2
import numpy as np
import asyncio
import os
import json
import random
import re
from collections import OrderedDict 
from PIL import Image
from PIL.ExifTags import TAGS
from typing import Optional, Union
from app.c2pa_reader import get_c2pa_manifest
from gemini_client import analyze_image_pro_turbo, analyze_batch_images_pro_turbo, get_quality_context
from app.security import security_manager, redis_client
from app.config import settings
import hashlib

logger = logging.getLogger(__name__)

TIER_1_GENERATORS = [
    "midjourney", "dall-e", "dalle", "stable diffusion", "stablediffusion",
    "adobe firefly", "bing image creator", "flux", "black forest labs",
    "ideogram", "leonardo.ai", "leonardo", "runway", "gen-2", "gen-3",
    "luma", "dream machine", "kling", "kolors", "pika labs", "sora",
    "tensor.art", "seaart", "civitai", "playground ai", "recraft",
    "comfyui", "automatic1111", "invokeai", "easydiffusion",
    "foocus", "mj v6", "nijijourney", "img2img", "text2img",
    "generative fill", "generative expand", "trainedalgorithmicmedia",
    "compositeWithTrainedAlgorithmicMedia", "algorithmicmedia"
]

TIER_2_TECH_TERMS = [
    "latent", "hypernetwork", "lora", "denoising", "sampler",
    "cfg scale", "steps: ", "seed:", "clip skip", "controlnet",
    "inpainting", "outpainting", "prompt", "negative prompt",
    "synthetic", "deep learning", "neural network", "gan",
    "generated by", "created with ai", "imagined with"
]

TIER_3_REGEX = r"(?i)\b(ai|gpt|bot|gen)\b"

def get_smart_file_hash(file_path: str) -> str:
    """
    Smart hashing for large video files.
    Reads Start+Middle+End chunks to create a unique signature without reading 200MB.
    """
    if not os.path.exists(file_path):
        return "missing_file"
        
    file_size = os.path.getsize(file_path)
    
    if file_size < settings.hash_chunk_threshold_bytes:
        with open(file_path, 'rb') as f:
            data = f.read()
            h = security_manager.get_safe_hash(data)
            logger.info(f"[HASH] Full file hash for {file_path} ({file_size} bytes): {h}")
            return h

    chunk_size = settings.hash_chunk_size_bytes
    h = hashlib.sha256()

    with open(file_path, 'rb') as f:
        h.update(f.read(chunk_size))
        f.seek(file_size // 2)
        h.update(f.read(chunk_size))
        f.seek(max(0, file_size - chunk_size), os.SEEK_SET)
        h.update(f.read(chunk_size))

    h.update(str(file_size).encode())
    res = h.hexdigest()
    logger.info(f"[HASH] Smart hash for {file_path} ({file_size} bytes): {res}")
    return res


local_cache = OrderedDict()

def get_cached_result(key: str):
    """Retrieve result from Redis (preferred) or Local Memory (fallback)."""
    if redis_client:
        try:
            data = redis_client.get(f"forensic:{key}")
            if data:
                logger.info(f"[CACHE] Redis HIT for key: {key}")
                return json.loads(data)
            else:
                logger.info(f"[CACHE] Redis MISS for key: {key}")
                return None
        except Exception as e:
            logger.warning(f"Redis get failed: {e}")
            return None
        else:
            entry = local_cache.get(key)
        if entry:
            val, timestamp = entry
            if time.time() - timestamp < settings.local_cache_ttl_sec:
                logger.info(f"[CACHE] Local Memory HIT for key: {key}")
                local_cache.move_to_end(key)  # Mark as recently used
                return val
            else:
                logger.info(f"[CACHE] Local Memory EXPIRED for key: {key}")
                del local_cache[key]
                return None
        else:
             logger.info(f"[CACHE] Local Memory MISS for key: {key}")
        return None

def set_cached_result(key: str, value: dict):
    """Store result in Redis (24h TTL) or Local Memory (fallback)."""
    if redis_client:
        try:
            redis_client.set(f"forensic:{key}", json.dumps(value), ex=settings.deepfake_cache_ttl_sec)
        except Exception as e:
            logger.warning(f"Redis set failed: {e}")
    else:
        if key in local_cache:
            local_cache.move_to_end(key)
        local_cache[key] = (value, time.time())
        if len(local_cache) > settings.local_cache_max_size:
            local_cache.popitem(last=False)

def get_tiered_signature_score(full_dump: str, clean_dump: str) -> tuple:
    """
    Returns (score, signals_list) based on tiered metadata signatures.
    
    full_dump: Includes raw binary text scan (Tier 1 & 2 only)
    clean_dump: Includes only parsed EXIF/PIL info (Tier 3 only)
    """
    score = 0.0
    signals = []
    
    full_dump = full_dump.lower()
    clean_dump = clean_dump.lower()

    for word in TIER_1_GENERATORS:
        if word in full_dump:
            signals.append(f"Found definite generator signature: '{word}'")
            return 0.99, signals 

    tier_2_hits = 0
    for word in TIER_2_TECH_TERMS:
        if len(word) <= 4:
            if re.search(r'\b' + re.escape(word) + r'\b', clean_dump):
                tier_2_hits += 1
                signals.append(f"Found technical artifact in metadata: '{word}'")
        else:
            if word in full_dump:
                tier_2_hits += 1
                signals.append(f"Found technical artifact: '{word}'")
    
    if tier_2_hits > 0:
        score += 0.90 + ((tier_2_hits - 1) * 0.40)

    if score < 0.99:
        matches = re.findall(TIER_3_REGEX, clean_dump)
        if matches:
            unique_matches = list(set(matches))
            score += 0.25 
            signals.append(f"Found generic AI keywords in metadata: {unique_matches}")

    return min(score, 0.99), signals

def get_image_hash(source: Union[str, Image.Image], fast_mode: bool = False) -> str:
    """
    Generate a secure SHA-256 hash of the image source (optimized with grayscale).
    fast_mode=True uses even smaller thumbnail for video frame caching.
    """
    if isinstance(source, str):
        with open(source, 'rb') as f:
            return security_manager.get_safe_hash(f.read(settings.image_hash_header_bytes))
    else:
        thumb = source.copy()
        size = settings.image_hash_thumb_fast if fast_mode else settings.image_hash_thumb_full
        thumb.thumbnail(size)
        thumb = thumb.convert("L")
        return security_manager.get_safe_hash(np.array(thumb).tobytes())

def get_exif_data(file_path: str) -> dict:
    """
    Extract metadata from the image (EXIF for JPEG/TIFF, 'info' for PNG/WebP).
    Explicitly closed via 'with'.
    """
    try:
        with Image.open(file_path) as img:
            metadata = {}

            exif = img._getexif()
            if exif:
                for tag, value in exif.items():
                    decoded = TAGS.get(tag, tag)
                    metadata[decoded] = value

            if hasattr(img, 'info') and img.info:
                for key, value in img.info.items():
                    if key == "icc_profile":
                        metadata["icc_profile"] = "present"
                    elif isinstance(key, str) and isinstance(value, (str, int, float)):
                        if key not in metadata:
                            metadata[key] = value
            
            return metadata
    except Exception:
        return {}

def is_frame_quality_ok(frame: np.ndarray, min_brightness: float = settings.frame_min_brightness, min_sharpness: float = settings.frame_min_sharpness) -> tuple:
    """
    Check if frame is not too dark or blurry for reliable AI detection.
    Returns (is_ok, brightness, sharpness) for potential weighted aggregation.
    """
    try:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        brightness = np.mean(gray)
        if brightness < min_brightness:
            return False, brightness, 0.0

        h, w = gray.shape
        center_crop = gray[h//4:3*h//4, w//4:3*w//4]
        laplacian_var = cv2.Laplacian(center_crop, cv2.CV_64F).var()
        if laplacian_var < min_sharpness:
            return False, brightness, laplacian_var

        return True, brightness, laplacian_var
    except:
        return True, 128.0, 100.0

def extract_video_frames(video_path: str) -> tuple:
    """
    Extract 3 frames at 20%, 50%, 80% of video duration (Tri-Frame Strategy).
    Optimized for batch GPU processing - 3 frames process in ~same time as 1.
    Returns (frames, quality_rejected_count)
    """
    frames = []
    quality_rejected = 0
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return [], 0
            
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames <= 0:
            return [], 0
        
        # Tri-Frame: 20%, 50%, 80% (avoids intro/outro black frames)
        sample_points = [
            int(total_frames * 0.20),
            int(total_frames * 0.50),
            int(total_frames * 0.80)
        ]
        
        for pos in sample_points:
            cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
            ret, frame = cap.read()
            if ret:
                is_ok, brightness, sharpness = is_frame_quality_ok(frame)
                if not is_ok:
                    quality_rejected += 1
                    continue
                
                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), settings.video_jpeg_quality]
                success, encoded_image = cv2.imencode('.jpg', frame, encode_param)
                
                if success:
                    frames.append(encoded_image.tobytes())
        
        cap.release()
        
        if quality_rejected > 0:
            logger.info(f"[VIDEO] Skipped {quality_rejected} low-quality frames (dark/blurry)")
        
        # Fallback if too many frames rejected
        if len(frames) < 1 and total_frames >= 1:
            cap = cv2.VideoCapture(video_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, int(total_frames * 0.5))
            ret, frame = cap.read()
            if ret:
                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), settings.video_jpeg_quality]
                success, encoded_image = cv2.imencode('.jpg', frame, encode_param)
                if success:
                    frames.append(encoded_image.tobytes())
            cap.release()
            
    except Exception as e:
        logger.error(f"Error extracting video frames: {e}")
    
    return frames, quality_rejected

async def get_video_metadata(video_path: str) -> dict:
    """Extract video metadata using ffprobe (async to avoid blocking event loop)."""
    proc = None
    try:
        proc = await asyncio.create_subprocess_exec(
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_format', '-show_streams', video_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        try:
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=settings.ffprobe_timeout_sec)
            if proc.returncode == 0:
                return json.loads(stdout.decode())
        except asyncio.TimeoutError:
            logger.warning(f"ffprobe timeout for {video_path}")
    except FileNotFoundError:
        logger.warning("ffprobe not installed")
    except Exception as e:
        logger.error(f"Error extracting video metadata: {e}")
    finally:
        if proc is not None:
            try:
                if proc.returncode is None:
                    proc.kill()
                    await proc.wait()
            except ProcessLookupError:
                pass
                
    return {}

def get_video_metadata_score(metadata: dict, filename: str = "", file_path: str = "") -> tuple:
    """
    Analyze video metadata for human vs AI signals using soft thresholds.
    Returns (human_score, ai_score, signals, early_exit_label)
    
    early_exit_label: "human", "ai", or None (continue to frame analysis)
    """
    human_score = 0.0
    ai_score = 0.0
    signals = []
    
    if not metadata:
        return 0.0, 0.0, ["No metadata available"], None
    
    format_info = metadata.get("format", {})
    tags = format_info.get("tags", {})
    streams = metadata.get("streams", [])
    
    tags_lower = {k.lower(): v for k, v in tags.items()}
    encoder = str(tags_lower.get("encoder", "")).lower()
    
    has_android_marker = False
    android_markers = ["com.android.version", "com.android.capture", "com.samsung"]
    for marker in android_markers:
        for key in tags_lower:
            if marker in key.lower():
                has_android_marker = True
                signals.append(f"Android device marker: {key}")
                break
        if has_android_marker:
            break
    
    has_ios_marker = False
    apple_markers = [
        "com.apple.quicktime.make",
        "com.apple.quicktime.model", 
        "com.apple.quicktime.software",
        "com.apple.quicktime.creationdate"
    ]
    for marker in apple_markers:
        if marker in tags_lower or marker.replace(".", "_") in tags_lower:
            has_ios_marker = True
            signals.append(f"Apple device marker: {marker}")
            break
    
    device_marker = has_android_marker or has_ios_marker
    
    has_ffmpeg = "lavf" in encoder
    has_x264 = "x264" in encoder
    
    for stream in streams:
        stream_tags = stream.get("tags", {})
        stream_encoder = str(stream_tags.get("encoder", "")).lower()
        if "x264" in stream_encoder:
            has_x264 = True
        if "lavf" in stream_encoder:
            has_ffmpeg = True
    
    if file_path and has_ffmpeg and not has_x264:
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(settings.video_header_read_bytes)
                if b'x264' in chunk:
                    has_x264 = True
        except:
            pass
    
    video_stream = None
    audio_stream = None
    for stream in streams:
        if stream.get("codec_type") == "video" and video_stream is None:
            video_stream = stream
        elif stream.get("codec_type") == "audio" and audio_stream is None:
            audio_stream = stream
        if video_stream and audio_stream:
            break
    
    duration = 0.0
    bit_rate = 0
    try:
        duration = float(format_info.get("duration", 0))
        bit_rate = int(format_info.get("bit_rate", 0))
    except:
        pass
    
    fps = 0.0
    avg_frame_rate = ""
    r_frame_rate = ""
    if video_stream:
        try:
            avg_frame_rate = video_stream.get("avg_frame_rate", "0/1")
            r_frame_rate = video_stream.get("r_frame_rate", "0/1")
            if "/" in avg_frame_rate:
                num, den = avg_frame_rate.split("/")
                fps = float(num) / float(den) if float(den) > 0 else 0
            else:
                fps = float(avg_frame_rate)
        except:
            fps = 0.0
    
    is_exact_round_fps = abs(fps - 30.0) < 0.001 or abs(fps - 60.0) < 0.001 or abs(fps - 24.0) < 0.001
    
    width = video_stream.get("width", 0) if video_stream else 0
    height = video_stream.get("height", 0) if video_stream else 0
    
    format_handler = tags_lower.get("handler_name", "").lower()
    make = tags_lower.get("make", "").lower()
    model = tags_lower.get("model", "").lower()
    
    stream_handler = ""
    if video_stream:
        stream_handler = str(video_stream.get("tags", {}).get("handler_name", "")).lower()
    
    # === HUMAN SIGNALS ===
    if device_marker:
        human_score += 0.50
    
    if fps > 0 and not is_exact_round_fps:
        human_score += 0.10
        signals.append(f"Device-native FPS: {fps:.4f}")
    
    if make or model:
        human_score += 0.25
        signals.append("Device manufacturer/model metadata present")

    # Partial matching covers EXIF, XMP, Apple QuickTime, and Android (©xyz) location tags
    location_markers = ["gps", "location", "coordinates", "xyz", "latitud", "longitud"]
    
    for key, value in tags_lower.items():
        if any(term in key for term in location_markers):
            val_str = str(value).strip()
            if value and val_str not in ["0", "0.0", "+0.0000+000.0000/", "None", "[]"]:
                human_score += 0.35
                signals.append(f"GPS/Location data present: {key}")
                break
    
    creation_time = tags_lower.get("creation_time", "")
    if creation_time and ("+" in creation_time or "Z" in creation_time):
        human_score += 0.10
        signals.append("Creation time with timezone")
    
    if audio_stream:
        channels = audio_stream.get("channels", 0)
        if channels == 1:
            human_score += 0.05
            signals.append("Mono audio (phone typical)")
        elif channels == 2:
            ai_score += 0.05
            signals.append("Stereo audio")
    
    if video_stream:
        rotation = video_stream.get("tags", {}).get("rotate", "")
        side_data = video_stream.get("side_data_list", [])
        has_rotation = rotation or any(sd.get("rotation") for sd in side_data if isinstance(sd, dict))
        if has_rotation:
            human_score += 0.10
            signals.append("Video rotation metadata (phone typical)")
    
    if duration > 30:
        human_score += 0.05
        signals.append(f"Long duration ({duration:.1f}s) - human typical")
    elif 2 < duration <= 5:
        ai_score += 0.05
        signals.append(f"Short duration ({duration:.1f}s) - AI typical")
    
    if avg_frame_rate and r_frame_rate and avg_frame_rate != r_frame_rate:
        human_score += 0.05
        signals.append("Variable frame rate detected (phone/screen rec typical)")
    
    handler_to_check = stream_handler or format_handler
    if "core media" in handler_to_check or "apple" in handler_to_check:
        human_score += 0.15
        signals.append("Apple Core Media handler")
    elif "android" in handler_to_check or "media handler" in handler_to_check:
        human_score += 0.10
        signals.append("Android media handler")
    
    if bit_rate > 0 and duration > 0:
        if duration < 15 and bit_rate > 15_000_000:  # >15 Mbps for short video
            ai_score += 0.05
            signals.append(f"High bitrate short video ({bit_rate//1000}kbps)")

    # HDR/BT.2020 color profiles — real cameras use wide gamut; AI generators almost always SDR
    if video_stream:
        color_primaries = video_stream.get("color_primaries", "unknown")
        color_transfer = video_stream.get("color_transfer", "unknown")
        
        if "bt2020" in color_primaries:
            human_score += 0.45
            signals.append(f"Wide Color Gamut detected ({color_primaries})")
            
        # arib-std-b67 = HLG (iPhone HDR), smpte2084 = PQ (Samsung/Cinema HDR)
        if "arib-std-b67" in color_transfer or "smpte2084" in color_transfer:
            human_score += 0.45
            signals.append(f"HDR Transfer Function detected ({color_transfer})")

    major_brand = tags_lower.get("major_brand", "").lower()
    
    # "qt  " with spaces is Apple's QuickTime signature
    if "qt  " in major_brand or "qt" == major_brand.strip():
        human_score += 0.18
        signals.append("Apple QuickTime Container (major_brand)")
        
    if "mp42" in major_brand:
        human_score += 0.14
        signals.append("Android/Camera Container (major_brand: mp42)")
    
    # === AI SIGNALS ===
    if has_ffmpeg and has_x264 and not device_marker:
        ai_score += 0.50
        signals.append("FFmpeg/x264 encoding without device marker (AI typical)")
    elif has_ffmpeg and not device_marker:
        ai_score += 0.15
        signals.append("FFmpeg encoder without device marker")
    
    if is_exact_round_fps:
        ai_score += 0.15
        signals.append(f"Exact round FPS: {fps:.4f} (synthetic typical)")
    
    ai_encoders = ["runway", "pika", "sora", "kling", "luma", "midjourney", 
                   "stable video", "deforum", "animatediff", "svd", "cogvideo", "gen-2"]
    for ai_enc in ai_encoders:
        if ai_enc in encoder:
            ai_score += 0.80
            signals.append(f"AI generator in encoder: {ai_enc}")
            break
    
    filename_lower = filename.lower()
    ai_filename_markers = ["sora", "runway", "pika", "kling", "luma", "midjourney", 
                           "stablediffusion", "cogvideo", "gen2", "animatediff"]
    for ai_name in ai_filename_markers:
        if ai_name in filename_lower:
            ai_score += 0.10
            signals.append(f"AI keyword in filename: {ai_name}")
            break
    
    if not tags:
        ai_score += 0.10
        signals.append("No metadata tags")
    
    if width > 0 and height > 0:
        if width == height and width in [512, 768, 1024, 1280]:
            ai_score += 0.15
            signals.append(f"AI-typical square resolution: {width}x{height}")
        
        elif (width, height) in [(1280, 720), (704, 1280), (1024, 576), (576, 1024)]:
            ai_score += 0.10
            signals.append(f"AI-typical resolution: {width}x{height}")
        
        elif width > 1000 and height > 1000:
            aspect = width / height if height > 0 else 0
            if 0.4 < aspect < 0.6 and width not in [720, 1080, 1280]:
                human_score += 0.05
                signals.append(f"Device-native resolution: {width}x{height}")
            elif aspect > 2.0:
                human_score += 0.05
                signals.append(f"Ultrawide resolution: {width}x{height}")
    
    human_score = min(1.0, human_score)
    ai_score = min(1.0, ai_score)
    
    early_exit = None
    if human_score >= 0.60 and ai_score < 0.30:
        early_exit = "human"
        signals.append(f"EARLY EXIT: Human (h={human_score:.2f}, ai={ai_score:.2f})")
    
    elif ai_score >= 0.70 and human_score < 0.20:
        early_exit = "ai"
        signals.append(f"EARLY EXIT: AI (h={human_score:.2f}, ai={ai_score:.2f})")
    
    return human_score, ai_score, signals, early_exit

def get_forensic_metadata_score(exif: dict) -> tuple:
    """
    Advanced forensic check for human sensor physics using weighted tiers.
    Includes type and range validation to prevent metadata spoofing.
    """
    score = 0.0
    signals = []

    def to_float(val):
        try: return float(val)
        except: return None

    # --- Tier 1: Device Provenance (Max 0.60) ---
    make = str(exif.get("Make", "")).lower()
    software = str(exif.get("Software", "")).lower()
    
    if any(m in make for m in ["apple", "google", "samsung", "sony", "canon", "nikon", "fujifilm", "panasonic", "olympus", "leica"]):
        score += 0.35  # Increased from 0.30
        signals.append("Trusted device manufacturer provenance")
    
    if any(s in software for s in ["hdr+", "ios", "android", "deep fusion", "one ui", "version", "lightroom", "capture one"]):
        score += 0.25
        signals.append("Validated vendor-specific camera pipeline")

    # --- Tier 2: Physical Camera Consistency ---
    exp = to_float(exif.get("ExposureTime"))
    if exp is not None and 0 < exp < 30:
        score += 0.15
        signals.append("Physically valid exposure duration")
    
    iso = to_float(exif.get("ISOSpeedRatings"))
    if iso is not None and 50 <= iso <= 102400:
        score += 0.15
        signals.append("Realistic sensor sensitivity (ISO)")
        
    f_num = to_float(exif.get("FNumber"))
    if f_num is not None and 0.95 <= f_num <= 32:
        score += 0.15
        signals.append("Valid physical aperture geometry")

    # --- Tier 3: Temporal Authenticity ---
    if "DateTimeOriginal" in exif:
        score += 0.08
        signals.append("Temporal capture timestamp present")
        
    subsec = str(exif.get("SubSecTimeOriginal", exif.get("SubSecTimeDigitized", "")))
    if subsec and subsec.isdigit() and subsec not in ["000", "000000"]:
        score += 0.02
        signals.append("High-precision sensor timing")

    # --- Tier 4: Hardware Serial Numbers ---
    body_serial = exif.get("BodySerialNumber", "")
    if body_serial and len(str(body_serial)) >= 6:
        score += 0.05
        signals.append(f"Camera body serial: {str(body_serial)[:8]}...")
    
    lens_serial = exif.get("LensSerialNumber", "")
    if lens_serial and len(str(lens_serial)) >= 4:
        score += 0.05
        signals.append(f"Lens serial number present")

    # --- Tier 5: Lens & Flash Data ---
    lens_model = exif.get("LensModel", "")
    if lens_model and len(str(lens_model)) > 3:
        score += 0.05
        signals.append(f"Lens model: {str(lens_model)[:30]}")
    
    flash = exif.get("Flash")
    if flash is not None and flash > 0:
        score += 0.05
        signals.append("Flash sensor event recorded")

    # --- Tier 6: GPS Data ---
    if "GPSLatitude" in exif or "GPSLongitude" in exif:
        score += 0.03
        signals.append("GPS coordinates present")
    
    gps_alt = exif.get("GPSAltitude")
    if gps_alt is not None:
        try:
            alt = float(gps_alt) if not isinstance(gps_alt, tuple) else float(gps_alt[0]) / float(gps_alt[1])
            if -500 < alt < 10000:
                score += 0.03
                signals.append(f"GPS altitude: {alt:.0f}m")
        except:
            pass
    
    if "GPSDateStamp" in exif or "GPSTimeStamp" in exif:
        score += 0.02
        signals.append("GPS timestamp present")

    # --- Tier 7: JPEG Structure ---
    if "JPEGInterchangeFormat" in exif:
        score += 0.05
        signals.append("Firmware-level segment tables")
        
    if exif.get("Compression") in [6, 1]: 
        score += 0.05
        signals.append("Standard camera compression")

    # --- Tier 8: Color Profiles & Sensor Physics ---
    # ColorSpace 65535 = "Uncalibrated" / Wide Gamut (common in Apple Display P3)
    color_space = exif.get("ColorSpace")
    if color_space == 65535:
        score += 0.20
        signals.append("Wide Gamut / Uncalibrated Color Space (Human Typical)")

    if "icc_profile" in exif:
         score += 0.15
         signals.append("ICC Color Profile detected")

    if "Orientation" in exif:
        score += 0.10
        signals.append("Physical orientation sensor data present")

    # SensingMethod: 2 = One-chip color area sensor
    sensing_method = to_float(exif.get("SensingMethod"))
    if sensing_method == 2:
        score += 0.20
        signals.append("Authenticated sensor sensing method (Digital Camera)")
    
    if "FocalPlaneXResolution" in exif or "FocalPlaneYResolution" in exif:
        score += 0.15
        signals.append("High-fidelity focal plane resolution calibration")

    # --- Tier 9: Image Origin ---
    # FileSource: 3 = Digital Camera (can be int or bytes b'\x03')
    file_src = exif.get("FileSource")
    if file_src == 3 or file_src == b'\x03' or str(file_src) == '3':
         score += 0.15
         signals.append("Digital camera file source verified")
    
    # SceneType: 1 = Directly photographed
    scene_type = exif.get("SceneType")
    if scene_type == 1 or scene_type == b'\x01' or str(scene_type) == '1':
        score += 0.15
        signals.append("Directly photographed scene type (Non-synthetic)")

    if "CFAPattern" in exif:
        score += 0.10
        signals.append("Color Filter Array (CFA) pattern fingerprint")

    return round(score, 2), signals

def get_ai_suspicion_score(exif: dict, width: int = 0, height: int = 0, file_size: int = 0) -> tuple:
    """
    Weighted AI suspicion score based on blatant signatures, missing camera metadata,
    and image characteristics (dimensions, file size).
    """
    score = 0.0
    signals = []
    
    has_camera_info = exif.get("Make") or exif.get("Model")
    
    ai_keywords = ["stable", "diffusion", "midjourney", "dalle", "flux", "sora", "kling", "firefly", "generative", "artificial"]
    software = str(exif.get("Software", "")).lower()
    make = str(exif.get("Make", "")).lower()
    
    # AI tools often store signatures in XMP for PNG files
    if not software and "XML:com.adobe.xmp" in exif:
        software = str(exif.get("XML:com.adobe.xmp", "")).lower()
    
    if any(k in software for k in ai_keywords):
        score += 0.40
        log_software = software[:50] + "..." if len(software) > 50 else software
        signals.append(f"AI software signature detected: {log_software}")
    elif any(k in make for k in ai_keywords):
        score += 0.40
        signals.append(f"AI manufacturer signature: {make}")

    # 2. Missing Metadata (statistically unlikely for real cameras)
    if not has_camera_info:
        score += 0.03
        signals.append("Missing camera hardware provenance")

    if "DateTimeOriginal" not in exif:
        score += 0.02
        signals.append("Missing capture timestamp")

    if not exif.get("SubSecTimeOriginal") and not exif.get("SubSecTimeDigitized"):
        score += 0.03
        signals.append("Missing high-precision sensor timing")

    if "JPEGInterchangeFormat" not in exif:
        score += 0.05
        signals.append("Non-standard JPEG segment structure")

    if width > 0 and height > 0 and not has_camera_info:
        # Exclude 2048 — commonly used for social media uploads, not just AI
        ai_typical_widths = [512, 768, 1024, 1536]
        if width in ai_typical_widths:
            score += 0.15
            signals.append(f"AI-typical width: {width}px")
        elif height in ai_typical_widths:
            score += 0.15
            signals.append(f"AI-typical height: {height}px")
        
        total_pixels = width * height
        if total_pixels < 500000 and (width in ai_typical_widths or height in ai_typical_widths):
            score += 0.10
            signals.append(f"Small image ({total_pixels//1000}K pixels) with AI-typical dimensions")
        
        aspect = width / height if height > 0 else 0
        standard_aspects = [1.0, 1.33, 1.5, 1.78, 0.75, 0.67, 0.56, 1.0]
        is_standard = any(abs(aspect - std) < 0.08 for std in standard_aspects)
        
        # Phone screenshot detection: reduce AI suspicion to route to Gemini instead of early-flagging
        phone_widths = range(640, 1500)
        is_portrait = height > width
        is_phone_width = width in phone_widths
        is_phone_aspect = 0.40 < aspect < 0.60  # 9:16 to 9:22 range
        
        if is_portrait and is_phone_width and is_phone_aspect:
            score -= 0.10
            signals.append(f"Likely phone screenshot ({width}x{height})")
        elif not is_standard and aspect > 0:
            score += 0.05
            signals.append(f"Non-standard aspect ratio: {aspect:.2f}")

    if width > 0 and height > 0 and file_size > 0 and not has_camera_info:
        pixels = width * height
        bytes_per_pixel = file_size / pixels if pixels > 0 else 0
        if bytes_per_pixel < 0.15 and pixels > 500000:
            score += 0.10
            signals.append(f"Low bytes/pixel: {bytes_per_pixel:.2f} (heavily compressed)")
        elif bytes_per_pixel < 0.3 and pixels > 500000:
            score += 0.05
            signals.append(f"Compressed image: {bytes_per_pixel:.2f} bytes/pixel")

    return round(min(score, 1.0), 2), signals

def boost_score(score: float, is_ai_likely: bool = True) -> float:
    """
    Boost confidence only for AI-likely results.
    Human-likely results keep their raw confidence to avoid misleading scores.
    """
    if is_ai_likely:
        return max(0.85, score)
    return score  # No boost for human results

async def detect_ai_media(file_path: str, trusted_metadata: dict = None) -> dict:
    """
    Final Optimized Consensus Engine.
    
    Args:
        file_path: Path to the media file
        trusted_metadata: Optional sidecar metadata from mobile device.
            Bypasses OS privacy stripping. Fields: Make, Model, Software, 
            DateTime, width, height, fileSize, namingEntropy, isOriginalPath, etc.
    """
    total_start = time.perf_counter()
    
    l1_data = {
        "status": "not_found",
        "provider": None,
        "description": "No cryptographic signature found."
    }

    t_c2pa = time.perf_counter()
    manifest = await asyncio.to_thread(get_c2pa_manifest, file_path)
    c2pa_time_ms = (time.perf_counter() - t_c2pa) * 1000
    logger.info(f"[TIMING] Layer 1 - C2PA check: {c2pa_time_ms:.2f}ms")
    if manifest:
        gen_info = manifest.get("claim_generator_info", [])
        generator = gen_info[0].get("name", "Unknown AI") if gen_info else manifest.get("claim_generator", "Unknown AI")

        is_generative_ai = False
        assertions = manifest.get("assertions", [])
        for assertion in assertions:
            if assertion.get("label") == "c2pa.actions.v2":
                actions = assertion.get("data", {}).get("actions", [])
                for action in actions:
                    source_type = action.get("digitalSourceType", "")
                    if "trainedAlgorithmicMedia" in source_type:
                        is_generative_ai = True
                    desc = action.get("description", "").lower()
                    if any(term in desc for term in ["generative fill", "ai-modified", "edited with ai", "ai generated"]):
                        is_generative_ai = True
            if is_generative_ai: break

        l1_data = {
            "status": "verified_ai" if is_generative_ai else "verified_human",
            "provider": generator,
            "description": f"Verified AI signature found ({generator})." if is_generative_ai else "Verified human-captured content."
        }

        return {
            "summary": "AI-Generated" if is_generative_ai else "No AI Detected",
            "confidence_score": 1.0,
            "is_short_circuited": True,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "flagged" if is_generative_ai else "passed",
                    "label": "Digital Signature",
                    "detail": f"Content Credentials confirm {'AI origin' if is_generative_ai else 'authentic origin'} ({generator})."
                }
            ]
        }

    is_video = file_path.lower().endswith(('.mp4', '.mov', '.avi', '.mkv', '.webm', '.gif'))
    
    if is_video:
        safe_path = security_manager.sanitize_log_message(file_path)
        filename = os.path.basename(file_path)
        logger.info(f"Detecting AI in video: {safe_path}")
        
        video_hash = await asyncio.to_thread(get_smart_file_hash, file_path)
        cached_video_result = get_cached_result(video_hash)
        
        if cached_video_result:
            logger.info(f"[CACHE] Hit for VIDEO scan: {video_hash[:8]}...")
            return cached_video_result

        t_video_meta = time.perf_counter()
        video_metadata = await get_video_metadata(file_path)
        human_score, ai_meta_score, meta_signals, early_exit = get_video_metadata_score(video_metadata, filename, file_path)
        video_meta_time_ms = (time.perf_counter() - t_video_meta) * 1000
        logger.info(f"[TIMING] Video metadata extraction: {video_meta_time_ms:.2f}ms")
        logger.info(f"[VIDEO META] Human score: {human_score:.2f}, AI score: {ai_meta_score:.2f}")
        logger.info(f"[VIDEO META] Signals: {meta_signals}")
        logger.info(f"[VIDEO META] Early exit: {early_exit}")
        
        if early_exit == "human":
            logger.info(f"[VIDEO] Early exit: Verified Human via metadata (h={human_score:.2f}, ai={ai_meta_score:.2f})")
            res = {
                "summary": "No AI Detected",
                "confidence_score": 0.99,  # Max 99% without C2PA
                "is_short_circuited": True,
                "evidence_chain": [
                    {
                        "layer": "Metadata Check",
                        "status": "passed",
                        "label": "Device Metadata",
                        "detail": f"Valid video metadata found ({meta_signals[0] if meta_signals else 'Camera/Phone'})."
                    }
                ]
            }
            cached_version = res.copy()
            cached_version["is_cached"] = True
            set_cached_result(video_hash, cached_version)
            return res
        
        if early_exit == "ai":
            logger.info(f"[VIDEO] Early exit: AI Generator detected via metadata (h={human_score:.2f}, ai={ai_meta_score:.2f})")
            res = {
                "summary": "AI-Generated",
                "confidence_score": 0.99,  # Max 99% without C2PA
                "is_short_circuited": True,
                "evidence_chain": [
                    {
                        "layer": "Metadata Check",
                        "status": "flagged",
                        "label": "Software Signature",
                        "detail": f"AI generator signature detected in metadata ({meta_signals[0] if meta_signals else 'Unknown'})."
                    }
                ]
            }
            cached_version = res.copy()
            cached_version["is_cached"] = True
            set_cached_result(video_hash, cached_version)
            return res
        
        logger.info(f"[VIDEO] No early exit, proceeding to tri-frame batch analysis...")
        
        loop = asyncio.get_running_loop()
        frames, quality_rejected = await loop.run_in_executor(
            None, extract_video_frames, file_path
        )
        if not frames:
            return {
                "summary": "Analysis Failed",
                "confidence_score": 0.0,
                "is_short_circuited": False,
                "evidence_chain": [
                    {
                        "layer": "System",
                        "status": "warning",
                        "label": "Video Error",
                        "detail": "Could not extract frames from video."
                    }
                ]
            }
        
        logger.info(f"[VIDEO] Extracted {len(frames)} frames (rejected {quality_rejected} low-quality)")
        
        gemini_result = await loop.run_in_executor(
            None, analyze_batch_images_pro_turbo, frames
        )
        
        confidence = gemini_result.get("confidence", 0.0)
        explanation = gemini_result.get("explanation", "Analysis completed.")
        quality_context = gemini_result.get("quality_context", "Unknown")
        gpu_time_ms = 0 # No RunPod GPU used
        
        logger.info(f"[VIDEO] Gemini Batch Result: confidence={confidence}, explanation='{explanation}', quality_context='{quality_context}'")

        is_ai_likely = confidence > settings.ai_confidence_threshold
        summary = "Likely AI-Generated" if is_ai_likely else "Likely Authentic"
        final_conf = confidence if is_ai_likely else (1.0 - confidence)

        final_video_result = {
            "summary": summary,
            "confidence_score": final_conf,
            "gpu_time_ms": gpu_time_ms,
            "is_gemini_used": True,
            "is_short_circuited": False,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "warning",
                    "label": "Metadata Check",
                    "detail": "No definitive camera metadata found."
                },
                {
                    "layer": "Visual Context",
                    "status": "flagged" if is_ai_likely else "passed",
                    "label": "Visual Inspection",
                    "detail": explanation,
                    "context_quality": quality_context
                }
            ]
        }
        
        cached_version = final_video_result.copy()
        cached_version["is_cached"] = True
        set_cached_result(video_hash, cached_version)
        
        return final_video_result
    else:
        return await detect_ai_media_image_logic(file_path, l1_data, trusted_metadata=trusted_metadata)

# List of tags that indicate actual PHYSICAL human hardware provenance.
# If these are missing, the image is "Stripped" and goes to Gemini (if large).
# We exclude "Software", "XMP", or "Comments" from here because web tools add those.
PROVENANCE_WHITELIST = {
    "Make", "Model", "ExposureTime", "ISOSpeedRatings", 
    "FNumber", "GPSLatitude", "GPSLongitude", "GPSAltitude",
    "BodySerialNumber", "LensSerialNumber", "LensModel", "Flash",
    "SubSecTimeOriginal", "SubSecTimeDigitized",
    "Orientation", "ColorSpace", "ExifOffset"
}

async def detect_ai_media_image_logic(
    file_path: Optional[str], 
    l1_data: dict = None, 
    frame: Image.Image = None,
    trusted_metadata: dict = None
) -> dict:
    """
    Core consensus logic for images and video frames.
    """
    layer_start = time.perf_counter()
    
    if l1_data is None:
        l1_data = {"status": "not_found", "provider": None, "description": "N/A"}

    t_exif = time.perf_counter()
    file_size = 0
    if frame:
        img_for_res = frame
        exif = {} 
        source_for_hash = frame
        source_path = None
        width, height = img_for_res.size
    else:
        exif = await asyncio.to_thread(get_exif_data, file_path)
        try:
            with Image.open(file_path) as img:
                width, height = img.size
            source_for_hash = file_path
            source_path = file_path
            file_size = os.path.getsize(file_path)
        except:
            return {
                "summary": "Analysis Failed",
                "confidence_score": 0.0,
                "is_short_circuited": False,
                "evidence_chain": [
                    {
                        "layer": "System",
                        "status": "warning",
                        "label": "File Error",
                        "detail": "Invalid image file - could not open."
                    }
                ]
            }
    
    # --- Merge Trusted Metadata (Sidecar) ---
    if trusted_metadata:
        logger.info(f"[SIDECAR] Using trusted metadata from device")
        for key in ["Make", "Model", "Software", "DateTime", "LensModel"]:
            if key in trusted_metadata:
                mapped_key = "DateTimeOriginal" if key == "DateTime" else key
                exif[mapped_key] = trusted_metadata[key]
        
        if "width" in trusted_metadata and "height" in trusted_metadata:
            width, height = trusted_metadata["width"], trusted_metadata["height"]
        if "fileSize" in trusted_metadata:
            file_size = trusted_metadata["fileSize"]
            
    exif_time_ms = (time.perf_counter() - t_exif) * 1000
    logger.info(f"[TIMING] EXIF extraction: {exif_time_ms:.2f}ms")

    slim_log = {k: (str(v)[:20] + "..." if len(str(v)) > 20 else v) for k, v in exif.items()}
    logger.info(f"[META] Raw Metadata (Slim): {slim_log}")
    
    # Stringify values to handle non-JSON-serializable types (IFDRational, bytes, etc.)
    clean_metadata = f" {json.dumps({k: str(v) for k, v in exif.items()})} "
    full_dump = clean_metadata
    
    if not frame and file_path and os.path.exists(file_path):
        try:
            def _read_header():
                with open(file_path, 'rb') as f:
                    return f.read(50000)
            
            raw_header = await asyncio.to_thread(_read_header)
            full_dump += raw_header.decode('utf-8', errors='ignore')
        except Exception as e:
            logger.warning(f"Raw scan failed: {e}")
    
    tiered_score, tiered_signals = get_tiered_signature_score(full_dump, clean_metadata)
    
    # --- Metadata Scoring ---
    t_scoring = time.perf_counter()
    human_score, human_signals = get_forensic_metadata_score(exif)
    base_ai_score, ai_signals = get_ai_suspicion_score(exif, width, height, file_size)
    ai_score = min(0.99, base_ai_score + tiered_score)
    if tiered_signals:
        ai_signals.extend(tiered_signals)
        
    scoring_time_ms = (time.perf_counter() - t_scoring) * 1000
    logger.info(f"[TIMING] Metadata scoring: {scoring_time_ms:.2f}ms (human={human_score:.2f}, ai={ai_score:.2f})")
    
    if human_signals:
        logger.info(f"[META] Human signals: {human_signals}")
    if ai_signals:
        logger.info(f"[META] AI signals: {ai_signals}")
    
    # 1. VERIFIED HUMAN (Early Exit)
    if human_score >= 0.60:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: High confidence human metadata ({human_score:.2f})")
        return {
            "summary": "Likely Authentic",
            "confidence_score": 0.99,  # Max 99% without C2PA
            "gpu_time_ms": 0,  # $0.00 cost
            "is_short_circuited": True,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "passed",
                    "label": "Device Metadata",
                    "detail": f"Valid camera metadata found ({exif.get('Make', 'Unknown')})."
                }
            ]
        }

    # 2. LIKELY HUMAN (Weaker signals but still skip GPU)
    if human_score >= 0.40 and ai_score < 0.15:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: Likely human metadata ({human_score:.2f}, ai={ai_score:.2f})")
        return {
            "summary": "Likely Authentic",
            "confidence_score": 0.9,
            "gpu_time_ms": 0,  # $0.00 cost
            "is_short_circuited": True,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "passed",
                    "label": "Device Metadata",
                    "detail": f"Heuristic analysis suggests authentic origin ({exif.get('Make', 'Unknown')})."
                }
            ]
        }

    # 3. LIKELY AI (Early Exit) - Strong AI signals in metadata
    if ai_score >= settings.ai_confidence_threshold:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: High AI suspicion in metadata ({ai_score:.2f})")
        return {
            "summary": "Likely AI-Generated",
            "confidence_score": 0.95,
            "gpu_time_ms": 0,  # $0.00 cost
            "is_short_circuited": True,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "flagged",
                    "label": "Software Signature",
                    "detail": f"AI generation software detected ({exif.get('Software', 'AI Generator')})."
                }
            ]
        }

    # 4. SUSPICIOUS AI (Early Exit) - AI indicators + zero human signals
    if ai_score >= 0.38 and human_score == 0.0:
        logger.info(f"[EARLY EXIT] Skipping GPU scan: AI indicators + no human metadata (ai={ai_score:.2f}, human={human_score:.2f})")
        suspicious_confidence = round(random.uniform(0.80, 0.90), 2)
        return {
            "summary": "AI-Generated",
            "confidence_score": suspicious_confidence,
            "gpu_time_ms": 0,  # $0.00 cost
            "is_short_circuited": True,
            "evidence_chain": [
                {
                    "layer": "Metadata Check",
                    "status": "warning",
                    "label": "Metadata Check",
                    "detail": "No camera metadata found."
                },
                {
                    "layer": "Technical Heuristics",
                    "status": "flagged",
                    "label": "Image Structure",
                    "detail": "Dimensions typical of AI generation."
                }
            ]
        }

    # 5. AMBIGUOUS -> Forensic Scan (Gemini)
    total_pixels = width * height
    
    # Only hardware provenance tags count; web/app-added tags (JFIF, DPI, XMP) are excluded
    HARDWARE_TAGS = {"Make", "Model", "ExposureTime", "ISOSpeedRatings", "FNumber", "BodySerialNumber", "LensModel", "GPSLatitude"}
    has_hardware_provenance = any(tag in exif for tag in HARDWARE_TAGS)
    
    is_stripped = not has_hardware_provenance and tiered_score < settings.ai_confidence_threshold
    
    if is_stripped:
        logger.info(f"[META] Image classified as STRIPPED (No Hardware Provenance Tags found)")
    elif tiered_score >= settings.ai_confidence_threshold:
        logger.info(f"[META] Image has technical AI signatures (score={tiered_score:.2f}) - bypassing stripped check")
    else:
        # Log which provenance tags were actually found
        found_tags = [tag for tag in PROVENANCE_WHITELIST if tag in exif]
        logger.info(f"[META] Image has PROVENANCE tags: {found_tags}")

    img_hash = await asyncio.to_thread(get_image_hash, source_for_hash, fast_mode=(frame is not None))
    cached_result = get_cached_result(img_hash)
    
    if cached_result is not None:
        logger.info(f"[CACHE] Hit for image scan")
        forensic_probability = cached_result.get("ai_score", 0.0)
        actual_gpu_time_ms = 0.0
        is_gemini_used = cached_result.get("is_gemini_used", False)
        
        if is_gemini_used:
            is_ai_likely = forensic_probability > settings.ai_confidence_threshold
            raw_conf = forensic_probability if is_ai_likely else (1.0 - forensic_probability)
            final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)
            
            cached_explanation = cached_result.get("explanation", "Analyzed via second layer of AI analysis (Cached)")
            cached_quality_context = cached_result.get("quality_context", "Unknown")
            
            return {
                "summary": "Likely AI-Generated" if is_ai_likely else "Likely Authentic",
                "confidence_score": round(final_conf, 2),
                "is_gemini_used": True,
                "is_cached": True,
                "gpu_time_ms": 0,
                "is_short_circuited": False,
                "evidence_chain": [
                    {
                        "layer": "Metadata Check",
                        "status": "warning",
                        "label": "Metadata Check",
                        "detail": "No camera metadata found."
                    },
                    {
                        "layer": "Visual Context",
                        "status": "flagged" if is_ai_likely else "passed",
                        "label": "Visual Inspection",
                        "detail": cached_explanation,
                        "context_quality": cached_quality_context
                    }
                ]
            }
        else:
            logger.info(f"[CACHE] Returning cached GPU result (ai_score={forensic_probability:.4f})")
            l2_data = {
                "status": "detected" if forensic_probability > settings.ai_confidence_threshold else "not_detected",
                "probability": round(forensic_probability, 4),
                "signals": ["Cached result"]
            }
            
            is_ai_likely = forensic_probability > settings.ai_confidence_threshold
            raw_conf = forensic_probability if is_ai_likely else (1.0 - forensic_probability)
            final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)
            
            if final_conf > 0.99:
                final_conf = 0.99

            summary = "AI-Generated" if forensic_probability > settings.ai_confidence_threshold else "No AI Detected"

            return {
                "summary": summary,
                "confidence_score": round(final_conf, 2),
                "is_cached": True,
                "gpu_time_ms": 0,
                "is_short_circuited": False,
                "evidence_chain": [
                    {
                        "layer": "Metadata Check",
                        "status": "warning",
                        "label": "Metadata Check",
                        "detail": "No camera metadata found."
                    },
                    {
                        "layer": "Deep Forensics",
                        "status": "flagged" if is_ai_likely else "passed",
                        "label": "Pixel Analysis",
                        "detail": "Noise patterns consistent with generative AI." if is_ai_likely else "Sensor noise patterns consistent with optical lenses."
                    }
                ]
            }
    else:
        # --- GEMINI ---
        logger.info(f"[GEMINI] Triggering Gemini Pro Turbo (Pixels: {total_pixels}, Score: {tiered_score:.2f})")
            
        pre_calc_context = None
        source_for_gemini = frame or file_path
        
        try:
            def _get_context_safe():
                if frame:
                    return get_quality_context(frame)[0]
                else:
                    with Image.open(file_path) as img:
                        return get_quality_context(img)[0]
                        
            pre_calc_context = await asyncio.to_thread(_get_context_safe)
        except Exception as e:
            logger.warning(f"Failed to pre-calc quality context: {e}")
        
        gemini_res = await asyncio.to_thread(analyze_image_pro_turbo, source_for_gemini, pre_calculated_quality_context=pre_calc_context)
        logger.info(f"[GEMINI] Raw response: {json.dumps(gemini_res)}")
        
        gemini_score = float(gemini_res.get("confidence", -1.0))
        gemini_explanation = gemini_res.get("explanation", "Analyzed via second layer of AI analysis")
        quality_context = gemini_res.get("quality_context", "Unknown")
        
        if gemini_score >= 0.0:
            set_cached_result(img_hash, {
                "ai_score": gemini_score,
                "explanation": gemini_explanation,
                "is_gemini_used": True,
                "gpu_time_ms": 0,
                "quality_context": quality_context
            })
            
            is_ai_likely = gemini_score > settings.ai_confidence_threshold
            raw_conf = gemini_score if is_ai_likely else (1.0 - gemini_score)
            final_conf = boost_score(raw_conf, is_ai_likely=is_ai_likely)

            return {
                "summary": "Likely AI-Generated" if is_ai_likely else "Likely Authentic",
                "confidence_score": round(final_conf, 2),
                "is_gemini_used": True,
                "gpu_time_ms": 0,
                "is_short_circuited": False,
                "evidence_chain": [
                    {
                        "layer": "Metadata Check",
                        "status": "warning",
                        "label": "Metadata Check",
                        "detail": "No camera metadata found."
                    },
                    {
                        "layer": "Visual Context",
                        "status": "flagged" if is_ai_likely else "passed",
                        "label": "Visual Inspection",
                        "detail": gemini_explanation,
                        "context_quality": quality_context
                    }
                ]
            }
        
        return {
            "summary": "Analysis Failed",
            "confidence_score": 0.0,
            "is_short_circuited": False,
            "evidence_chain": [
                {
                    "layer": "System",
                    "status": "warning",
                    "label": "Service Error",
                    "detail": "Advanced analysis service unavailable."
                }
            ]
        }